```{r include = FALSE}
options(scipen=999)
```

## Homeworked Example
[Screencast Link](https://youtu.be/T5XpWmpjO-M)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introductory lesson](https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html#introduction-to-the-data-set-used-for-homeworked-examples) in [ReCentering Psych Stats](https://lhbikos.github.io/ReCenterPsychStats/). An .rds file which holds the data is located in the [Worked Examples](https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples) folder at the GitHub site the hosts the OER. The file name is *ReC.rds*.

The suggested practice problem for this chapter is to evaluate the measurement model that would precede the evaluation of a structural model. And actually, we will need to evaluate two measurement models -- an "all items" on indicators model and a parceled model.


###  Identify the structural model you will evaluate {-} 

It should have a minimum of three variables and could be one of the prior path-level models you already examined   
 
I will repeat the simple mediation that I suggested in path analysis. Specifically, I hypothesize that the evaluation of socially responsive pedagogy will be predicted by intentional recentering through traditional pedagogy.

X = Centering: explicit recentering (0 = precentered; 1 = recentered)
M = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable)
Y = SRPed:  socially responsive pedagogy (continuously scaled with higher scores being more favorable)

### Specify a research model {-}

I am hypothesizing that the evaluation of social responsive pedagogy is predicted by intentional recentering through traditional pedagogy. 

### Import the data and format the variables in the model {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <- readRDS("ReC.rds")
```

I don't need to score my scales, but it is important to know what they are:

TradPed has 5 items:  ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
SRPed has 4 items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
Centering is 1 item -- it's a factor with two levels pre, re.

I can create a babydf with just those items.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf <- dplyr::select(raw, Centering, ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)
```

Let's check the structure of the variables:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
str(babydf)
```
The centering variable will need to be dummy coded as 0/1:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf$CENTERING <- as.numeric(babydf$Centering)
babydf$CENTERING <- (babydf$CENTERING - 1)
str(babydf)
```

        
### Specify and evaluate a measurement model with all items as indicators {-} 


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
init_msmt_mod <- "
        ##measurement model
         CTR =~ CENTERING #this is a single item indicator, I had to add code below to set the variance

         TradPed =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
         
         SRPed =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         
        # Variance of the single item indicator
        CENTERING ~~ 0*CENTERING
        
        # Covariances
         CTR ~~ TradPed
         CTR ~~ SRPed
         TradPed ~~ SRPed
        "

set.seed(230916)
init_msmt_fit <- lavaan::cfa(init_msmt_mod, data = babydf, missing = "fiml")
init_msmt_fit_sum <- lavaan::summary(init_msmt_fit, fit.measures = TRUE, standardized = TRUE)
init_msmt_fit_sum


```

### Interpret the results {-}   

Criteria                                            | Our Results                         | Criteria met?|
|:--------------------------------------------------|:-----------------------------------:|:------------:|
|Factor loadings significant, strong, proper valence|all $p < 0.001$, lowest = .582(SRPed)|Yes           | 
|Non-significant chi-square                         |$\chi ^{2}(33)= 178.307, p < 0.001$  |No            |  
|$CFI\geq .95$ (or at least .90)                    |CFI = 0.911                          |Almost!       |  
|$RMSEA\leq .05$ (or < .08, at least < .10, also 90CI)|RMSEA = 0.119, 90CI[0.102,	 0.137] |No            |  
|$SRMR\leq .08$ (at least < .10)                    |SRMR = 0.060                         |Yes           | 
|Combination rule: $CFI \geq .95$ & $SRMR \leq .08$ |CFI = 0.911, SRMR = 0.060            |No            |


Results were mixed. Here is the fit string:  $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
semPlot::semPaths(init_msmt_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```

Results of the evaluation of the measurement model can be exported as .csv files with the following code. These produce output that inglude global fit indices, parameter estimates, and correlations between the latent variables, respetively. 
The *tidySEM::table_fit* function will display all of the global fit indices.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
init_msmt_fitstats <- tidySEM::table_fit(init_msmt_fit)
write.csv(init_msmt_fitstats, file = "init_msmt_fitstats.csv")
#parameter estimates
init_msmt_pEsts <- tidySEM::table_results(init_msmt_fit, digits=3, columns = NULL)
write.csv(init_msmt_pEsts, file = "init_msmt_pEsts.csv")
#correlations between latent variables
init_msmt_LVcorr <- tidySEM::table_cors(init_msmt_fit, digits=3)
write.csv(init_msmt_LVcorr, file = "init_msmt_LVcorr.csv")

```


### Specify and evaluate a measurement model with either the subscale or randomly assigned to 3 parcels approaches {-}

Each dataset has its unique challenges. This one has relatively few items per scale. I will therefore:

* Retain the single item indicator for CENTERING.
* Randomly assign the 5 items of the TradPed scale to 3 parcels (for parcels with 2, 2, and 1 member each)
  - I don't actually know that this is the best solution, but I will do it for demonstration purposes.
* Retain the four items as indicators for SRPed.

Here I assign the TradPed items to the 3 parcels.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(230916)
items <- c('ClearResponsibilities', 'EffectiveAnswers', 'Feedback', 'ClearOrganization', 'ClearPresentation')
parcels <- c("p1_TR", "p2_TR","p3_TR")
data.frame(items = sample(items),
           parcel = rep(parcels, length = length(items)))  
```
I can now create the parcels using the traditional scoring procedure. I want both items to be present to score, so I will leave the .75 requirement (as a placeholder).

As a variable, *ClearResponsibilities* will stand alone (i.e., the scoring mechanism won't work on a single variable).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
TRp1_vars <- c('ClearPresentation','EffectiveAnswers')
TRp2_vars <- c('Feedback','ClearOrganization')

babydf$p1_TR <- sjstats::mean_n(babydf[, ..TRp1_vars], .75)
babydf$p2_TR <- sjstats::mean_n(babydf[, ..TRp2_vars], .75)

#If the scoring code above does not work for you, try the format below which involves removing the periods in front of the variable list. One example is provided.
#babydf$p3_TR <- sjstats::mean_n(babydf[, TRp3_vars], .75)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
parc_msmt_mod <- "
        ##measurement model
         CTR =~ CENTERING #this is a single item indicator, I had to add code below to set the variance

         TradPed =~ p1_TR + p2_TR + ClearResponsibilities
         
         SRPed =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         
        # Variance of the single item indicator
        CENTERING ~~ 0*CENTERING
        
        # Covariances
         CTR ~~ TradPed
         CTR ~~ SRPed
         TradPed ~~ SRPed
        "

set.seed(230916)
parc_msmt_fit <- lavaan::cfa(parc_msmt_mod, data = babydf, missing = "fiml")
parc_msmt_fit_sum <- lavaan::summary(parc_msmt_fit, fit.measures = TRUE, standardized = TRUE)
parc_msmt_fit_sum
```


### Interpret the results {-}

Criteria                                            | Our Results                         | Criteria met?|
|:--------------------------------------------------|:-----------------------------------:|:------------:|
|Factor loadings significant, strong, proper valence|all $p < 0.001$, lowest = .430(SRPed)|Yes           | 
|Non-significant chi-square                         |$\chi ^{2}(18)= 112.982, p < 0.001$  |No            |  
|$CFI\geq .95$ (or at least .90)                    |CFI = 0.925                          |Almost!       |  
|$RMSEA\leq .05$ (or < .08, at least < .10, also 90CI)|RMSEA = 0.130, 90CI[0.108,	0.154]  |Worsened!     |  
|$SRMR\leq .08$ (at least < .10)                    |SRMR = 0.062                         |Worsened!     | 
|Combination rule: $CFI \geq .95$ & $SRMR \leq .08$ |CFI = 0.925, SRMR = 0.062            |No            |


Results were mixed and somewhat worse than the initial model that included all item-level indicators. Here is the fit string:  $\chi^2(18)= 112.982, p < 0.001, CFI =  0.925, RMSEA = 0.130, 90CI[0.108,	 0.154], SRMR = 0.062$


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
semPlot::semPaths(parc_msmt_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
parc_msmt_fitstats <- tidySEM::table_fit(parc_msmt_fit)
write.csv(parc_msmt_fitstats, file = "parc_msmt_fitstats.csv")
#parameter estimates
parc_msmt_pEsts <- tidySEM::table_results(parc_msmt_fit, digits=3, columns = NULL)
write.csv(parc_msmt_pEsts, file = "parc_msmt_pEsts.csv")
#correlations between latent variables
parc_msmt_LVcorr <- tidySEM::table_cors(parc_msmt_fit, digits=3)
write.csv(parc_msmt_LVcorr, file = "parc_msmt_LVcorr.csv")

```
### Make notes about similarities and differences in the all-items and parceled approaches {-} 

* Chi square was significant for both. A statistically significant chi-square is bad -- it means that our measurement model covariance matrix is statistically significantly different than the sample covariance matrix.
* The CFI improved a tiny bit for the parcelled model.
* The RMSEA and SRMR worsened a tiny bit for the parcelled model.

Given that I didn't really have enough items to parcel and the fit worsened for some of the indicators for the parcelled model, I think I will stay with the measurement model with item-level indicators. When I created the scales for the course evaluations, I was mindful of degrees of freedom and identification status of the model and intentionally chose to have a few items.


### APA style results with table and figure {-}

Analyzing our proposed mediation model followed the two-step procedure of first evaluating a measurement model with acceptable fit to the data and then proceeding to test the structural model. Each latent variable was represented by each of the items on its subscale. Given that TradPed and SRPed had 5 and 4 items, respectively, we did not parcel items. The Centering variable with two levels (pre-centered, re-centered) was recoded as a dummy variable with 0, 1 coding. In the specification, its measurement error was fixed at zero. While all factor loadings were strong, statistically significant, and properly valanced, global fit statistics were mixed: $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$. We proceeded to testing the strutural model with caution.


Table 1  

|Factor Loadings for the Measurement Model 
|:-------------------------------------------------------------|

|                         
|:----------------------------:|:-----:|:----:|:-----:|:------:|
| Latent variable and indicator|est    |SE    | *p*   |est_std |

|
|:-----------------------------|:-----:|:----:|:-----:|:------:|
|**Traditional Pedagogy**      |       |      |       |        |
|ClearResponsibilities	       |1.000	 |0.000	|      	|0.845   |
|EffectiveAnswers	             |0.967	 |0.056	|<0.001	|0.815   |
|Feedback	                     |0.915	 |0.063	|<0.001	|0.725   |
|ClearOrganization	           |1.193	 |0.075	|<0.001	|0.771   |
|ClearPresentation	           |1.111	 |0.063	|<0.001	|0.841   |
|**Socially Responsive Pedagogy**|     |      |       |        |
|InclusvClassrm	               |1.000	 |0.000	|     	|0.702   |
|EquitableEval	               |0.953	 |0.087	|<0.001|0.717    |
|MultPerspectives	             |1.466	 |0.116	|<0.001	|0.839   |
|DEIintegration	               |0.901	 |0.099	|<0.001	|0.582   |
|**CENTERING**	               |0.000	 |0.000	|       |0.000   |


### Explanation to grader {-}






