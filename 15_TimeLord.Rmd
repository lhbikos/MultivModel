
# Calendrical Time (and Missingness) in MLMs {#TimeLord}

[Screencasted Lecture Link](https://youtube.com/playlist?list=PLtz5cFLQl4KO7qBcFjvQr6KYURru65nBI&si=BIKtcWmaTLMyryKg) 
 
```{r include=FALSE}
options(scipen=999)#eliminates scientific notation
```

Because much longitudinal research involves date or time stamps (and multi-level analyses can be improved by using unstructured time), the primary focus of this lecture is on working with *calendrical time.* We will consider what is the proper "metric" for clocking time, engage in "date math," and then rework the Lefevor et al.[@lefevor_religious_2017] with this alternate metric for time. The bonus reel of the chapter asks a number of "What if...?" questions (i.e., what if we used the Index (time structured) variable as an indicator of time; what if our dataset was unbalanced) and provides quick answers and offers possible R script solutions.

## Navigating this Lesson

There is about 1 hour and 20  minutes of lecture.  If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReC_CPA) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Learning objectives from this lecture include the following:

* Select proper "metric" of time (selecting between days, weeks, months, year).
* Conduct "date math":
  * add or subtract time to a date
  * create intervals between dates
* Compare results when different time metrics are used
* Compare the process and results when designs differ on balance/unbalance (i.e., varying numbers of observations per case)
  * Generate ideas for problems that may arise from imbalance

### Planning for Practice

The suggestions for practice in this lesson are quite flexible. I encourage you to try one or more that is relevant to the type of data you collect and the challenges you have when you use it. There are three basic suggestions:

* Rework the problem in the lesson but change the metric by which time is clocked from weeks to something else (e.g., hours, months). Compare the results. For more of a challenge, use the simulated data where depression (not anxiety) is the outcome and work through the entire process of preparing and analyzing calendrical time with that data.
* Using the data in this chapter or the depression-outcome data from the earlier lesson, randomly delete data from the long file. Compare the results of this unbalanced design to results with the complete data.
* Using data to which you have access, experiment with time and balance in a manner that makes sense ot you.

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Singer, J. D., & Willett, J.B. (2003). Chapter 5:  Treating time more flexibly.  In, Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY: Oxford University Press.
* Ford, C. (2017). Working with dates and time in R using the lubridate package.  University of Virginia Library:  Research Data Services + Sciences.  https://data.library.virginia.edu/working-with-dates-and-time-in-r-using-the-lubridate-package/ 
* Lefevor, G. T., Janis, R. A., & Park, S. Y. (2017). Religious and sexual identities: An intersectional, longitudinal examination of change in therapy. *The Counseling Psychologist, 45*(3), 387â€“413. https://doi-org.ezproxy.spu.edu/10.1177/0011000017702721

### Packages

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#will install the package if not already installed
#if(!require(lme4)){install.packages("lme4")}
#if(!require(sjStats)){install.packages("sjStats")}
#if(!require(sjPlot)){install.packages("sjPlot")}
#if(!require(formattable)){install.packages("formattable")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(robumeta)){install.packages("robumeta")}
#if(!require(scales)){install.packages("scales")}
#if(!require(psych)){install.packages("psych")}
#if(!require(data.table)){install.packages("data.table")}
#if(!require(ggplot2)){install.packages("ggplot2")}
#if(!require(lubridate)){install.packages("lubridate")}
```

## Exploring Variants of Time and Balance

Datasets present their unique challenges. In longitudinal designs where time is a focal predictor, the "clock" used to mark time is a critical choice. Further, there are a number of considerations when converting dates to markers of time.

Another challenge has to do with balance. A highlight of MLM is the hope that individuals can have missing observations and varying numbers of observations per case. In this chapter, we randomly delete a number of cases from the otherwise perfectly balanced dataset to see how it challenges (or doesn't) the process and the results.

Given that this chapter does not introduce a new statistic, let's get right to the example.

## Research Vignette

Our research vignette [@lefevor_religious_2017] examines the intersection of religious and sexual identities of clients in therapy. With 12,825 participants from the Center for Collegiate Mental Health 2012-2014 data set, the project is an example of working with *big data.* Because the data is available to members only (and behind a paywall), I simulated the data. In the simulation, categorical variables (e.g., sexual identity, session number, religious identity) were rendered as continuous variables and in the simulation, I needed to transform them back into categorical ones. Inevitably, this will have introduced a great deal of error. Thus, we can expect that the results from the simulated data will be different from those obtained by the authors. 

The Method section of the article provides detailed information about the inclusion criteria ofr the study and the coding of the variables. This included data about the religious and sexual identities as well as a minimum of three separate scores on the Counseling Center Assessment of Psychologial Sympsoms (CCAPS); [@locke_development_2012] measure. For the final dataset, clients attended an average of 10.58 sessions (*SD* = 7.65) and had an average of 5.36 CCAPS administrations (*SD* = 4.04).  This means that in the original dataset, each client was represented by a varying number of observations (likely ranging from 3 [the minimum required for inclusion] and, perhaps as many as 17 [adding +3*SD*s to the mean CCAPS administrations]). In simulating the data, I specified five observations for each of the 12,825 clients.

```{r  echo = FALSE, results='hide'}
5.36 + 4.04*(3)
```

Let's take a look at the variables in the study

* **Anxiety and Depression**:  The anxiety and depression ratings were taken from the CCAPS measure [@locke_development_2012] that assesses psychological distress across seven domains. Clients rate themselves over the past two weeks on a 5-point Likert-type scale ranging from 0 (*not at all like me*) to 4 (*extremely like me*). Higher scores indicate more distress. The dataset comes from multiple institutions with different procedures around assessment CCAPS there is not a 1:1 correspondence with session number and CCAPS assessment.

* **Sexual Identity**:  Sexual identity was dichotomized into heterosexual (-1, 85.5%) and LGBQQ (1, 14.5%).

* **Relious Identity**:  Religious identity was coded into three categories including dominant religious (DR; Christian, Catholic), nondominant religious (NDR; Muslim, Hindu, Buddhist, Jewish), and nondominant unaffiliated (NDU; agnostic, atheist, no preference).  The three categories were contrast coded with an orthogonal contrast-coding scheme with two variables.  The first variable compared DR(coded as 2) to NDU and NDR (coded as -1); the second variable compared the two nondominant groups (NDU = -1, DR = 0, NDR = 1). 

* **Time**:  Time was a variable in the study. In the article, Lefevor et al. [-@lefevor_religious_2017] clocked time with session number. Conceptualizing session as an indicator of "dose", each participants' observation included a session variable representing a value ranging from the first to the twentieth session.  In the prior lessons where we used this vignette, we centered the first session at 0 so that in the data simulation, session was an integer-level variable ranging from 1 to 19.   In the simulation for this lesson (where the goal is to manipulate time), I have further transformed session so that it is a date (year, month, date).

### Simulating the data from the journal article

This simulation is very similar (but not identical) to the simulation used in the MLM longitudinal exploration and model building chapters.  The primary difference is that when I created the SessionT variable, I did not require the values to be integers. Instead, the number of sessions could be fractional as they still ranged from 0 to 19. The session variable becomes our key in creating a variable representing *calendrical time*.

```{r  tidy=TRUE, tidy.opts=list(width.cutoff=70), results='hide'}
library(tidyverse)
set.seed(200513)
n_client = 12825
n_session = 5
b0 = 2.03 #intercept for anxiety
b1 = -.22 #b weight for L1 session
b2 = .13 #b weight for L2 sexual identity
b3 =  -.03 #b weight for L2 Rel1 (D-R vs ND-R & ND-U)
b4 = .01 #b weight for the L2 Rel2 (ND-R vs ND-U)
#the values used below are the +/- 3SD they produce continuous variables which later need to be transformed to categorical ones; admittedly this introduces a great deal of error/noise into the simulation
#the article didn't include a correlation matrix or M/SDs so this was a clunky process 
( Session = runif(n_client*n_session, -3.61, 3.18)) #calc L1 Session, values are the +/3 3SD
( SexualIdentity = runif(n_client*Session, -6.66, 6.92)) #calc L2 Sexual Identity, values are the +/3 3SD
( Religion1 = runif(n_client*Session, -3.43, 3.37)) #calc L2 Religion1, values are the +/3 3SD
( Religion2 = rep (runif(n_session, -3.38, 3.41), each = n_session)) #calc L2 Religion2, values are the +/3 3SD
mu = 1.76 #intercept of empty model 
sds = 2.264 #this is the SD of the DV
sd = 1 #this is the observation-level random effect variance that we set at 1

#( church = rep(LETTERS[1:n_church], each = n_mbrs) ) #this worked in the prior
( client = rep(LETTERS[1:n_client], each = n_session) )
#( session = numbers[1:(n_client*n_session)] )
( clienteff = rnorm(n_client, 0, sds) )
( clienteff = rep(clienteff, each = n_session) )
( sessioneff = rnorm(n_client*n_session, 0, sd) )
( Anxiety = b0 + b1*Session + b2*SexualIdentity + b3*Religion1 + b4*Religion2 + clienteff + sessioneff)
( dat = data.frame(client, clienteff, sessioneff, Session, SexualIdentity, Religion1, Religion2, Anxiety) )

dat <- dat %>% dplyr::mutate(ID = row_number())
#moving the ID number to the first column; requires 
dat <- dat%>% dplyr::select(ID, everything())

Lefevor2017 <- dat%>%
  select(ID, client, Session, SexualIdentity, Religion1, Religion2, Anxiety)

Lefevor2017$ClientID <- rep(c(1:12825), each = 5)
#rounded Sexual Identity into dichotomous variable
#85% were heterosexual, 

#The following variables should be L2, but were simulated as if they were L1
Lefevor2017$Rel1 <- as.numeric(robumeta::group.mean(Lefevor2017$Religion1,Lefevor2017$ClientID))#aggregated at group mean
Lefevor2017$Rel2 <- as.numeric(robumeta::group.mean(Lefevor2017$Religion2,Lefevor2017$ClientID))#aggregated at group mean
Lefevor2017$SxID <- as.numeric(robumeta::group.mean(Lefevor2017$SexualIdentity,Lefevor2017$ClientID))#aggregated at group mean

#Rel2 has contrast codes for dominant religion (DR, 0), nondominant religious (NDR, 1) and nondominant unspecified (NDU, -1)
#Strategy is to figure out the raw score associated with the percentile rank of  -1 and 0, to set the breakpoints for the coding
#NDU coded as -1
#19.2+13.5+9.6
#NDU has bottom 42.3 percent

#DR coded as 0, so quantile cut will be 42.3 + 52.7 = 95th
#33.4 + 19.3
#52.7% of sample (according to article) was DR
#must look up percentile ranks for 5% and 57.5%

#NDR
#2.3+1+1+.7
#NDR has 5% of sample
#42.3+52.7
#quantile(Lefevor2017$Religion2, probs = c(.423, .95))
#effects coding the second Religion variable so that NDU = -1, DR = 0, NDR = 1
Lefevor2017$Rel2L2 <- ifelse(Lefevor2017$Religion2 <= -3.0877087, -1, 
                             ifelse(Lefevor2017$Religion2 >= -3.0877087 & Lefevor2017$Religion2 <= 0.9299491, 0,1))

#checking work
#Rel2L2_table <- table(Lefevor2017$Rel2L2)
#prop.table(Rel2L2_table)
#Lefevor2017 %>%
#count(Rel2L2)

#creating the first religion variable where DR is 2 and NDR and NDU are both -1
Lefevor2017$Rel1L2 <- plyr::mapvalues(Lefevor2017$Rel2L2, from = c(-1, 0, 1), to = c(-1, 2, -1))
Lefevor2017$DRel0 <- plyr::mapvalues(Lefevor2017$Rel2L2, from = c(-1, 0, 1), to = c(1, 0, 1))

#checking to make sure that 52.7% are coded 2 (DR)
#Rel1L2_table <- table(Lefevor2017$Rel1L2)
#prop.table(Rel1L2_table)

#heterosexual is -1
#LGBTQIA+ is 1
#quantile(Lefevor2017$SxID, probs = c(.85))
Lefevor2017$SexID <- ifelse(Lefevor2017$SxID <= 1.203468, -1,1)
Lefevor2017$Het0 <- plyr::mapvalues(Lefevor2017$SexID, from = c(-1,1), to = c(0,1))
#SexID_table <- table(Lefevor2017$SexID)
#prop.table(SexID_table)

#creating a variable representing the session number for each client, in the article up to 20 sessions were allowed. 
#install.packages("scales")

#Right from the beginning I centered this so that 0 would represent intake
#Lefevor2017$Session0 <- as.integer(scales::rescale(Lefevor2017$Session, to = c(0, 19)))
Lefevor2017$SessionT <- scales::rescale(Lefevor2017$Session, to = c(0, 19))

#creating session waves (1 thru 5) by rank ordering within each person's variable the continuous variable Session that was created in the original simulation

LefevorTIME <- Lefevor2017%>%
  dplyr::group_by(ClientID) %>%
  dplyr::mutate(Index = rank(Session))

#selecting the simulated variables
LefevorTIME_sim <- LefevorTIME%>%
  dplyr::select(ClientID, Index, SessionT, Anxiety, DRel0, Het0)

#rearranging variables so that IDs are together
LefevorTIME_sim <- LefevorTIME_sim%>%
  dplyr::select(ClientID, Index, SessionT, Anxiety, DRel0, Het0)
#resorting data so that each person is together
LefevorTIME_sim <- dplyr::arrange(LefevorTIME_sim, ClientID, Index)

#In the transition from long-to-wide, it seems like you can only do one L1 variable at a time
#When there are multiple L1 and L2 vars, put all L2 vars on left of tilde
#The wave/index function should come next; this should be finite (like integers of 1,2,3,4) with a maximum
#Put the name of the SINGLE L1 variable in the concatonated list
library(data.table)
LfvrTWp1<-reshape2::dcast(LefevorTIME_sim, ClientID + DRel0 + Het0 ~ Index, value.var = c("Index"))
#rename the anxiety variable
LfvrTWp1<-  rename(LfvrTWp1, Index1 = "1", Index2 = "2", Index3 = "3", Index4 = "4", Index5 = "5")
LfvrTWp2<-reshape2::dcast(LefevorTIME_sim, ClientID ~ Index, value.var = c("Anxiety"))
#rename the anxiety variable
LfvrTWp2<-  rename(LfvrTWp2, Anx1 = "1", Anx2 = "2", Anx3 = "3", Anx4 = "4", Anx5 = "5")
#For remaining L1 variable, do them one at a time -- key them from the person-level ID and the wave/index.
LfvrTWp3<-reshape2::dcast(LefevorTIME_sim, ClientID ~ Index, value.var = c("SessionT"))
LfvrTWp3<-  dplyr::rename(LfvrTWp3, Sess1 = "1", Sess2 = "2", Sess3 = "3", Sess4 = "4", Sess5 = "5")
#Next, join the dataframes by the person-level ID
#Only two can be joined at a time
LfvrTWide <- dplyr::full_join(LfvrTWp1, LfvrTWp2, by = c("ClientID"))
LfvrTWide <- dplyr::full_join(LfvrTWide, LfvrTWp3,  by = c("ClientID"))
```

To increase the portability of the OER, this lesson uses simulated data. Here is script for exporting/downloading the long and wide forms of the data as a .csv files to your local computer and then importing/uploading it again. I find that saving the .csv file (data) in the same place as the .rmd file(s) is essential for R to connect the two.

Once you write the document to your file, you only need to run the piece of script that reads it back in.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Once you simulate the data, the "write.table" script saves it -- ideally to the folder where your .rmd file is located
#write.table(LefevorTIME_sim , file="LefevorTLong.csv", sep=",", col.names=TRUE, row.names=FALSE)
#If you have used the above code, you can clear your environment, then simply run this line of code to bring the data back in
#TIMElong <- read.csv ("LefevorTLong.csv", head = TRUE, sep = ",")
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Once you simulate the data, the "write.table" script saves it -- ideally to the folder where your .rmd file is located
write.table(LfvrTWide, file="LefevorTWide.csv", sep=",", col.names=TRUE, row.names=FALSE)
#If you have used the above code, you can clear your environment, then simply run this line of code to bring the data back in
#TIMEwide <- read.csv ("LefevorTWide.csv", head = TRUE, sep = ",")
```

## More Simulation -- Appointment Dates

Lefevor et al. [@lefevor_religious_2017] treated sessions as a dosage marker and clocked time by the number of sessions.  But what if we wanted to use calendrical time, as time?  In this lesson we will get practice at working with dates and calculating time intervals (e.g., days, weeks, or months) with those dates. To demonstrate another possible way of working with time, I have decided to simulate the dates as part of this lesson.

If you simulated the data and saved it locally, you should be able to bring it into the R Environment as a file. 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
TIMEwide <- read.csv ("LefevorTWide.csv", head = TRUE, sep = ",")
head(TIMEwide)
```

If we examine the head of the data, we see five variables (Sess1 to Sess5) that were resimulated to be fractional values between 0 and 19). Thinking this simulation through, we have five repeated measures from each client that correspond with when they took the CCAPS measure.Because counseling centers differed on the frequency and scheduling for the CCAPS, there is not a 1:1 correspondence with CCAPS administration and session number. I have kept this in mind in creating a "dates" based time metric.

First I must create the Intake date. In this particular simulation, this is simply a marker/baseline for clocking time.  I am only using it to create dates for the weekly counseling sessions.  In my imagination this might have been the date that the client called to schedule an appointment. 

In the script below, I am creating the variable "Intake."  I have assigned all the clients the same intake date (May 28, 2021).  I need to tell R that the format is "year/month/date."  The *ymd()* function does that for me.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
TIMEwide$Intake <- lubridate::ymd("2021-05-28")
```

We can confirm that our Intake variable is a date by checking the structure.

```{r}
str(TIMEwide$Intake)
```
In the next step of the data simulation I need to create Weeks# variables that have a date.  The date must consistently increase from Week1 through Week5. I am going to make the assumption that the simulated values for the Sess# variables (recall, they are fractional values that range from 0 to 19) are a reasonable estimate of *weekly* time.  Therefore, I can add it to the Intake date to create new dates for each of the five observations for each client. 
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
TIMEwide <- TIMEwide%>%
  dplyr::mutate (Weeks1 = Intake + lubridate::dweeks(Sess1))%>%
  dplyr::mutate (Weeks2 = Intake + lubridate::dweeks(Sess2))%>%
  dplyr::mutate (Weeks3 = Intake + lubridate::dweeks(Sess3))%>%
  dplyr::mutate (Weeks4 = Intake + lubridate::dweeks(Sess4))%>%
  dplyr::mutate (Weeks5 = Intake + lubridate::dweeks(Sess5))

head(TIMEwide)
```

Let's peek at the structure of one of the Weeks# variables.

```{r}
str(TIMEwide$Weeks1)
```
The POSIXct format stores a date (complete with time) and an associated timezone (i.e., the default time zone is the one your computer is set to). The metric is seconds beginning from January 1, 1970.  Any dates before that are in negative numbers.  This format makes it possible to do calculate with dates/times and then extract intervals.

To this point I have only been using some of the *lubridate* functions to simulate the data.  This is generally the point where we would receive the data -- that point where each event of interest has a time stamp.  

## Reworking Lefevor et al. using Calendrical Time (and Unbalanced Data)

### Creating Time Intervals

On their own, datestamps are not useful as predictors in a regression equation. Therefore, we must manipulate or transform them to represent some "sensible metric for time" [@singer_applied_2003]; that is, we need to think about how we wish to clock time. In making this consideration, we also want to think about how regression works. Time will be a focal predictor and so we are interested in the meaningfulness and interpretability of a "one-unit change." Using *days* as a metric seems too small because we would not expect to detect change in 1-day increments. Months feels reasonable to me, but with a maximum of 19 weeks, we would have a maximum of just under five units of the *months* metric. Because the span of time is only 19 session and sessions tend to occur weekly, let's create a metric of weeks. 

First, I need to create the *time intervals* from which I will extract the durations. In regression, it is important to have a meaningful zero point. We want Week1 to be that zero.

Calculating intervals of time requires using the %--% operator.  This results in a data column that lists the two dates being compared. In our example we want to count time forward by listing the Intake variable before the %--% and the Appointment variable.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(lubridate) #neeeded to use the %--% command
TIMEwide <- TIMEwide %>%
  dplyr::mutate(TimeInterval1 = Weeks1 %--% Weeks1)%>%
  dplyr::mutate(TimeInterval2 = Weeks1 %--% Weeks2)%>%
  dplyr::mutate(TimeInterval3 = Weeks1 %--% Weeks3)%>%
  dplyr::mutate(TimeInterval4 = Weeks1 %--% Weeks4)%>%
  dplyr::mutate(TimeInterval5 = Weeks1 %--% Weeks5)

head(TIMEwide)
```

Don't let the result of this script scare you! In the second step, we convert the TimeInterval# variable to weeks. The "d" in the "dweeks" function tells R that we are calculating a duration.  The "x=1" indicates that we are counting 1 week at a time.  If we wanted to create 2-week intervals, we simply write, "x = 2".

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
TIMEwide <- TIMEwide %>%
  dplyr::mutate(Wks1 = as.duration(TimeInterval1)/dweeks(x=1))%>%
  dplyr::mutate(Wks2 = as.duration(TimeInterval2)/dweeks(x=1))%>%
  dplyr::mutate(Wks3 = as.duration(TimeInterval3)/dweeks(x=1))%>%
  dplyr::mutate(Wks4 = as.duration(TimeInterval4)/dweeks(x=1))%>%
  dplyr::mutate(Wks5 = as.duration(TimeInterval5)/dweeks(x=1))

head(TIMEwide)
```
Let's check the structure of one of the Wks# variables.
```{r}
str(TIMEwide$Wks1)
```
Let's also take a look at the descriptives
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describe(TIMEwide[c("Wks1", "Wks2", "Wks3", "Wks4", "Wks5")])
```
The Wks1 through Wks5 variables are what we will use to clock time. We can see that they progress from 0 (hypothethically, the first appointment) to 18.90. This is consistent with the the original simulation, where I restricted the Session variable values were to range between 0 and 19.

This simulation isn't perfect in that the CCAPS administrations were not always at the first session. Additionally, there may be a few cases cases where the weeks values are extroardinarily close together or even in reverse order. A *time reversal* violates Singer and Willett's [-@singer_applied_2003] guidelines. None-the-less, in this simulation with over 12,000 cases, this should be sufficient for demonstration.  Further, it models some of the decisions made when we are working with raw data (which, at least in my lab, always has some complexities).

### Wide to Long

Now that our date math is finished, we can restructure the data from wide to long. We have created a number of variables related to time. Regarding the time variables will only need the Index (created automatically) and the Wks# variables.  Of course we will want our L1 (Anx#) and L2 (ClientID, DRel0, Het0) variables.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(data.table)
TIMElong <- (data.table::melt(setDT(TIMEwide), id.vars = c("ClientID", "DRel0", "Het0"), measure.vars =list(c("Anx1", "Anx2", "Anx3", "Anx4", "Anx5"),  c("Wks1", "Wks2", "Wks3", "Wks4", "Wks5") )))
#This process  does not preserve the variable names, so we need to rename them
TIMElong<- rename(TIMElong<-  rename(TIMElong, Index = variable, Anxiety = "value1", Weeks = "value2"))

#rearanging variables so that IDs are together
TIMElong <- TIMElong%>%
  select(ClientID, Index, Weeks, Anxiety, DRel0, Het0)
#resorting data so that each person is together
TIMElong <- arrange(TIMElong, ClientID, Index)

head(TIMElong)
```

```{r}
str(TIMElong)
```
Examining the structure of the data assures us that the value of the Weeks and Index variables progress in a positive direction, together. We also see that the Weeks variable is *numerical*. This is appropriate since we are simply counting the number of weeks.

### MLM is for unbalanced designs

In the prior lessons I noted that a strength of MLM is that data can be unbalanced. The original simulations produced perfectly balanced sets of data with 5 observations each.  To demonstrate that MLM can accommodate sets of data where the number of observations per person, vary.

Here, I update the simulation by randomly deleting 5000 rows. With this *big data* circumstance, this still leaves us a number of cases! **Note: this is for demonstration only. You would never, intentionally, delete cases without a compelling reason!**  

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(210529)
TIMElong <- TIMElong[-sample(1:nrow(TIMElong), 5000),]
```

### Abbreviated OLS Style Exploration

A more thorough preliminary investigation of the data occurs in a [prior lesson](#MLMexplore). In this quick demonstration, we take a look at individual growth plots from when clocked Weeks variable as nonparametric and parametric (i.e., where individual linear regressions are superimposed on the raw data).

We start by creating a random sample. Here is script allowing us to sample entire cases (i.e., the individual client with all their appointment data).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rndm30time <- subset(TIMElong, ClientID %in% sample(unique(TIMElong$ClientID), 30))
```

In these first two plots we look at anxiety as a function of the time-unstructured, weeks, data. Compared to the graphs in the chapter where the data was balanced (each person had 5 observations), it is apparent that not everyone has the same number of observations.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggplot(data = rndm30time, aes(x = Weeks, y = Anxiety)) + geom_point() + geom_line() +
    facet_wrap(~ClientID)
```

In the lesson on [longitudinal exploration](MLMexplore) we created individual regression equations and superimposed them on the raw data. This second plot does not print the individual regression equations for us, but calculates them behind the scenes and creates the individual plots. Similar to the plots with nonparametric smoothing, we notice the difference in numbers of observations per cases.  The straight line, though, does seem to be an appropriate functional form.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggplot(data = rndm30time, aes(x = Weeks, y = Anxiety)) + geom_point() +
    stat_smooth(method = "lm", se = FALSE) + facet_wrap(~ClientID)
```
If this were our real data, we would complete the process outlined in the [longitudinal exploration](MLMexplore) chapter.  Because the purpose of this lesson is focused on unstructured time with unbalanced data, we will move forward with a quick recap of building the MLM.  This next section is abbreviated from the more thorough process demonstrated in the [basic longitudinal model](LongMod) lesson.


### Rebuilding the Model (Unstructured Time, Unbalanced Design)

We follow the same model building approach as in the [basic longitudinal model](LongMod) lesson.  This involves progressing through the following models:

1. Unconditional means model
2. Unconditional growth model
3. Uncontrolled effects of sexual identity
4. Controlled effects of religious affiliation
5. Trimming nonsignificant effects

#### Model 1:  Unconditional means model

We start with the unconditional means model. In this model, we are seeking the grand mean. By identifying the nesting/grouping variable (ClientID) we also understand the proportions of between- and within- subjects variance.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
M1a <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
summary(M1a)

sjPlot::tab_model(M1a, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"


```

We learn that the mean anxiety score is 2.08 across all individuals and all times. A primary purpose of the unconditional means model is to obtain and interpret the ICC. Between-persons variance is 78%; within-persons variance is its inverse (22%).

Although I would normally run the diagnostic plots after each model, in this lesson I am only displaying them after the first model (as a quick check to make sure there is nothing catastrophically wrong) and after the last model.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#If the code below does not run, install the package glmmTMB from its source with the following code
#Remove the hashtag to install, then replace it to avoide re-re-installing the package
#install.packages("glmmTMB", type="source")

sjPlot::plot_model (M1a, type="diag")
```

#### Model 2:  Unconditional growth model

The second model is introduces time (this time our Weeks variable) into the L1 submodel.  Because a linear model made sense in our exploratory analyses, we specify a trajectory of linear change. We do not include any other substantive predictors.Because there are no other predictors, it is an *unconditional* growth model. 

**For comparison**, this was our prior model; we can see how the model building occurs.
M1a <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#with lme4 package
M2a <- lme4::lmer(Anxiety ~ Weeks +(Weeks | ClientID), TIMElong, REML = FALSE)
summary(M2a )

sjPlot::tab_model(M1a, M2a, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
A plot of predicted values illustrates the decrease in anxiety as sessions continue.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
sjPlot::plot_model (M2a, type="pred", terms=c("Weeks"))
```

**Interpreting Fixed Effects**
Looking at the *tab_model()* in the viewer, we can see that 

At Week1 (the zero value), the average client was estimated to have a non-zero anxiety level of 2.82 (*p* < 0.001; $\gamma _{00}$). On average, anxiety decreases by 0.08 units per session.

**Interpreting variance components**
With Weeks in the model, the L1 residual variance, $\sigma^{2}$ now summarizes the scatter of each person's data *around their own linear change trajectory*.

When we have both unconditional means and growth models, we can examine the decrease in within-person residual variance. Here's the formula:

$$Pseudo R_{\varepsilon }^{2} = \frac{\sigma^{2}(unconditional. means. model) - \sigma^{2}(unconditional. growth. model)}{\sigma ^{2}(unconditional. means. model)}$$

We calculate it manually:

```{r }
(1.45 - 1.26)/1.45
```

We conclude that 13% of the within-person variation in anxiety is explained by the number of weeks in therapy.  The only way to further reduce this variance component is to add time-covarying predictors to the L1 submodel.

The value of $\tau _{00}$ is 5.18; this is variance remaining around the intercept (anxiety at Week1). Because the introduction of the time variable, Weeks, changes the meaning of the L2 variance components, we do not compare the $\tau _{00}$) values between Models 1 and 2.  As we move forward (keeping Weeks in the model) we will use the Model 2 estimates as benchmarks for comparison.

The value of $\tau _{11}$  is 0.00; this is variance remaining around the slope (rate of growth).

#### Model 3:  The uncontrolled effects of sexual identity

We will add sexual identity into the model. Because the only other predictor is Weeks, its addition (as a L2 variable, with a cross-level interaction with time), it is considered to be an *uncontrolled* addition.

**For comparison**:
M1a <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
M2a <- lme4::lmer(Anxiety ~ Weeks +(Weeks | ClientID), TIMElong, REML = FALSE)
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#with lme4 package
M3a <- lme4::lmer(Anxiety ~ Weeks*Het0 +(Weeks | ClientID), TIMElong, REML = FALSE)
summary(M3a)

sjPlot::tab_model(M1a, M2a,M3a, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2", "Mod3"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
```{r}
sjPlot::plot_model (M3a, type="int", terms = c("Weeks", "Het0 [0,1]"))
```

**Interpreting Fixed Effects**
The estimated anxiety at Week1 for heterosexual individuals is 2.49 (*p* < .001). If the client is LGBQQ, the average Week1 anxiety level is higher: 2.77 (*p* < .001; 2.49 + .28).

```{r echo=FALSE, results='hide'}
2.49 + .28
```

The estimated rate of change in anxiety for the average client who is heterosexual is -.08 (*p* < .001) units per week. The estimated differential in the rate of change in anxiety between clients who are heterosexual and LGBQQ is nondistinguishable ($\beta$ = 0.00, *p* = .883). This is evident in the interaction plot produced by the *plot_model()* function.

**Interpreting Variance Components**

Not surprisingly, the $\sigma ^{2}$  value (1.26) stayed the same from M2a to M3a.  This is because we did not add a within-subjects (time covarying) predictor. If it had changed, we would have conducted the proportionate reduction in variance evaluation. 

$\tau _{00}$ decreased from 5.18 (M2a) to 5.164 (M3a). We can apply the proportionate reduction in variance formula to determine the proportion of L2 intercept variance accounted for by the Het0 addition.

$$Pseudo R_{\zeta }^{2} = \frac{\tau _{00} (unconditional. growth. model) - \tau _{00}(subsequent. model)}{\tau _{00}(unconditional. growth. model)}$$

```{r }
(5.18-5.16)/5.18
```

The $\tau _{00}$ variance component decreases by less than 1% from the unconditional growth model (M2a)

$\tau _{11}$  is unchanged.  Both M2a and M3a are 0.00; adding sexual identity did not change between-subjects' slopes. Similarly, if it had changed, we would have conducted the proportionate reduction in variance evaluation.

These variance components are now considered *partial* or *conditional* variances because they quantify the interindividual differences in change that remain unexplained by the model's predictors.  

#### Model 4:  The controlled effects of religious affiliation

Because sexual identity is already in the model, when we add religious affiliation we are controlling for the effects of sexual identity. Religious affiliation is an L2 variable. When we add it, we will also specify a cross-level interaction with Weeks (L1).

M1a <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
M2a <- lme4::lmer(Anxiety ~ Weeks +(Weeks | ClientID), TIMElong, REML = FALSE)
M3a <- lme4::lmer(Anxiety ~ Weeks*Het0 +(Weeks | ClientID), TIMElong, REML = FALSE)
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#with lme4 package
M4a <- lme4::lmer(Anxiety ~ Weeks*Het0 + Weeks*DRel0 + Het0*DRel0 + (Weeks | ClientID), TIMElong, REML = FALSE, control = lme4::lmerControl(optimizer= "bobyqa"))
summary(M4a)

sjPlot::tab_model(M1a, M2a, M3a, M4a, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2", "Mod3", "Mod4"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
We observe that none of the additional terms were statistically significant. Further, none of the variance components changed. Additionally, the output suggested convergence problems.  Let's trim.

M1a <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
M2a <- lme4::lmer(Anxiety ~ Weeks +(Weeks | ClientID), TIMElong, REML = FALSE)
M3a <- lme4::lmer(Anxiety ~ Weeks*Het0 +(Weeks | ClientID), TIMElong, REML = FALSE)
M4a <- lme4::lmer(Anxiety ~ Weeks*Het0 + Weeks*DRel0 + Het0*DRel0 + (Weeks | ClientID), TIMElong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))

#### Model 5:  Trimming non-signifcant effects

If we strictly trim all no-significant effects, we retain Weeks and sexual identity. We do not specify any interactions. 
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#with lme4 package
M5a <- lme4::lmer(Anxiety ~ Weeks + Het0 + (Weeks | ClientID), TIMElong, REML = FALSE, control = lme4::lmerControl(optimizer= "bobyqa"))
summary(M5a)

sjPlot::tab_model(M1a, M2a, M3a, M4a, M5a, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1", "Mod2", "Mod3", "Mod4", "Mod5"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```
```{r}
sjPlot::plot_model (M5a, type="diag")
```
Because of the significant trimming (of non-significant interaction effects), our plot resemble those we have observed throughout the model building process. For this final model we cannot use the *type = "int"* because it requires an interaction term and we did not have one.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
sjPlot::plot_model (M5a, type="pred",terms=c("Weeks", "Het0"))
```

Our results are consistent with what we expected when we trimmed.  That is, Anxiety for heterosexual clients at Week1 is 2.48; for LGBQQ clients it is 2.78. For all clients, anxiety decreases, on average, .08 per week. 

```{r}
2.49 + .29
```

## Structured Time: Reworking the Vignette with Index

So what would happen if we used the Index variable -- representing the five, sequenced, observations but ignoring the dosage (i.e., when marked time with Session# ranging from 0 to 19) or calendrical time (i.e., when we marked time in Weeks [fractional, ranging from 0 to 19]).

First, let's format Index as numeric.

```{r}
library(dplyr) 
TIMElong <- TIMElong %>%
    mutate(
        Index = as.numeric(Index)
    )

as.numeric(as.character(TIMElong$Index))
```

```{r}
str(TIMElong$Index)
```

Next, the abbreviated preliminary exploration. As you will see, the unbalanced design has prevented them from running properly. While MLM can accomodate unbalanced designs, it can cause difficulties here and there.  I will be looking for an alternate way to produce these graphs that can accommodate missing data more effectively.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(ggplot2)
ggplot(data = rndm30time, aes(x = Index, y = Anxiety)) + geom_point() + geom_line() +
    facet_wrap(~ClientID)
ggplot(data = rndm30time, aes(x = Index, y = Anxiety)) + geom_point() +
    stat_smooth(method = "lm", se = FALSE) + facet_wrap(~ClientID)
```

I will run all of the models in one chunk and produce the *tab_model* table.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#with lme4 package
M1i <- lme4::lmer(Anxiety ~1 +(1 | ClientID), TIMElong, REML = FALSE)
M2i <- lme4::lmer(Anxiety ~ Index + (Index | ClientID), TIMElong, REML = FALSE, control=lme4::lmerControl(check.nobs.vs.nRE="ignore"))
M3i <- lme4::lmer(Anxiety ~ Index*Het0 +(Index | ClientID), TIMElong, REML = FALSE, control = lme4::lmerControl(optimizer= "bobyqa"))
M4i <- lme4::lmer(Anxiety ~ Index*Het0 + Index*DRel0 + Het0*DRel0 + (Index | ClientID), TIMElong, REML = FALSE, control = lme4::lmerControl(optimizer= "bobyqa"))
M5i <- lme4::lmer(Anxiety ~ Index + Het0 + (Index | ClientID), TIMElong, REML = FALSE, control = lme4::lmerControl(optimizer= "bobyqa"))
summary(M5a)

sjPlot::tab_model(M1i, M2i, M3i, M4i, M5i, p.style = "numeric", show.ci = FALSE, show.se = TRUE, show.df = FALSE, show.re.var = TRUE, show.aic = TRUE, show.dev = TRUE, use.viewer = TRUE, dv.labels = c("Mod1i", "Mod2i", "Mod3i", "Mod4i", "Mod5i"))
#can swap this statement with the "file = "TabMod_Table"" to get Viewer output or the outfile that you can open in Word
#file = "TabMod_Table.doc"
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
sjPlot::plot_model (M5i, type="pred",terms=c("Index", "Het0"))
```
How do the models differ when Index (time-structured) is used to mark time?

In the fixed effects:

* All the models we evaluated came to similar conclusions. That is, the trimmed model included the time variable (SessNum, Weeks, or Index) and sexual identity as predictors.
* The effect of the sexual identity was comparable. That is, LGBQQ clients had anxiety levels that were about 0.28 points higher than heterosexual clients.
* The value of decline for the Index variable (0.25) was three times higher than the value for SessNum or Weeks (0.08). Given that the span of SessNum and weeks was about three times greater (i.e., 0 to 19 versus 0 to 5), this finding is comparable even though the numbers are different.  SessNum and Weeks are simply on different *clocks* than Index.

The patterns observed in the random effects are quite comparable across all three time variants.

## APA Style Writeup

In each lesson I like to describe or provide an example of an APA style write-up. Because this lesson was a demonstration of techniques and then a reworking of the research vignette in [longitudinal exploration](MLMexplore), I will refer you back to that lesson for an example of an APA Style write-up.

The only difference in writing it up would be to provide a description about how the Weeks variable clocked time and some detail about how it was calculated or coded.

## Residual and Related Questions...

..that you might have; or at least I had, but if had answered them earlier it would have disrupt the flow.

### What did we gain/lose by using Weeks or SessNum (unstructured) versus Index (structured) to mark time?

My strong preference is to use unstructured time. In this particular case, it "stretches out" the time scale so that we get a more precise understanding of change over time. While the effect of the time variable may not appear to be as strong (-.25 for Index; -0.08 for SessNum and Weeks), this is misleading.  Why?  The "clock" for Index has only 5 values.  In contrast, the clock for SessNum had 20 values (0 to 19, required to be integers/whole numbers) and the clock for Weeks was rather infinite (i.e., the range for Weeks was limited from 0 to 19 but it allowed for fractions).

### How did the unbalanced time impact the analysis and results?

The quality of data is always improved when there are at least three observations per case in longitudinal models.  Models *can* run when there are 1 or 2 observations per individual, but only in case where the rest of the data is really robust (this dataset is really robust!). Evidence, though, of problems caused by unequal cell sizes was observed when ggplot failed to render the individual growth plots. 

While our unbalanced design ran, I have had data where models would not converge. In those cases, convergence was made possible by (a) replacing control statements or optimizers and (b) deleting cases with only one observation per cases (and then if that didn't run, deleting cases with only two observations per case). 

Although we haven't yet addressed it in this OER, there are also options related to fixing and freeing the slopes and intercepts.
   
## Practice Problems

The prior lessons suggests working a longitudinal MLM from exploration through model building and interpretation. Consequently, for practice, I encourage further adaptation to that analysis by (a) changing the time metric and comparing analyses and (b) if using a simulation with no missingness, randomly delete some rows from the long file so that the numbers of cases are different. There are a number of ways to do this and all data are different. Consequently, the ideas below are merely suggestions.

### Problem #1: Rework this Lesson's Example by Changing the Time Metric to Days or Months 
If you would like practice with time conversions, rework this lesson by changing calendrical time from weeks (dweeks) to another unit such as days or months. Work the example and compare the answers to those in the lesson. For more of a challenge use the simulation from the Lefevor et al. [-@lefevor_religious_2017] where depression is the outcome. This will require you to engage in more of the simulation. The simulation is available in the Bonus Track of the [longitudinal exploration](MLMexplore)lesson. 

|Assignment Component  
|:---------------------------------------------------------------------------------|:-------------: |:------------:|
|1. Apply date format to variables                                                 |      5         |    _____     |
|2. Establish a "time interval" variable                                           |      5         |    _____     |
|3. Extract the duration from the variable                                         |      5         |    _____     |
|4. Convert to a long file                                                         |      5         |    _____     |
|5. Produce set of individual growth plots with an imposed regression line         |      5         |    _____     |    
|6. Run a set of models (unconditional means through trimming)                     |      5         |    _____     |
|7. Create a tab_model table with the final set of models                          |      5         |    _____     |
|8. Create a figure to represent the result                                        |      5         |    _____     |    
|9. Interpret the output, comparing how it differs from a different metric of time |      5         |    _____     | 
|10. Explanation to grader                                                         |      5         |    _____     |   
|**Totals**                                                                        |      50        |    _____     |

### Problem #2: Compare balanced and unbalanced designs

Using (a) this lesson's data or (b) the simulation where depression is the outcome (located in the Bonus Track of the [longitudinal exploration](MLMexplore)lesson), delete a different number of cases from the long file and rework the problem.

|Assignment Component  
|:---------------------------------------------------------------------------------|:-------------: |:------------:|
|1. From the long file, delete a meaningful number of cases                        |      5         |    _____     |
|2. Produce set of individual growth plots with an imposed regression line         |      5         |    _____     |    
|3. Run a set of models (unconditional means through trimming)                     |      5         |    _____     |
|4. Create a tab_model table with the final set of models                          |      5         |    _____     |
|5. Create a figure to represent the result                                        |      5         |    _____     |    
|6. Interpret the output, comparing how it differs from a different metric of time |      5         |    _____     | 
|7. Explanation to grader                                                          |      5         |    _____     |   
|**Totals**                                                                        |      35        |    _____     |

             
### Problem #3: Experiment with time and balance/unbalance in data that is available to you

Using data for which you have permission and access play around with time and balance/unbalance. 

|Assignment Component  
|:---------------------------------------------------------------------------------|:-------------: |:------------:|
|1. Play with time?__________                                                      |      5         |    _____     |
|2. Play with missingness?_________                                                |      5         |    _____     |
|3. Produce set of individual growth plots with an imposed regression line         |      5         |    _____     |    
|4. Run a set of models (unconditional means through trimming)                     |      5         |    _____     |
|5. Create a tab_model table with the final set of models                          |      5         |    _____     |
|6. Create a figure to represent the result                                        |      5         |    _____     |    
|7. Interpret the output, comparing how it differs from a prior analysis           |      5         |    _____     | 
|8. Explanation to grader                                                          |      5         |    _____     |   
|**Totals**                                                                        |      40        |    _____     |

## Bonus Track: 

![Image of a filmstrip](images/film-strip-1.jpg){#id .class width=620 height=211}

### FAQs

#### What if this was a Qualtrics datestamp?

Qualtrics date stamps look like this:  2018-01-03 10:05:46

Correspondingly, it is important to let lubridate know that the format is year/month/day_hour/minute/second. The script below, denotes that with the function *ymd_hms()*.  If your downloaded Qualtrics data does not have all those elements (or they are in different order), then consult the *lubridate* package options for the correct function. 

Below, hashtagged out, is code I have used to format the Qualtrics datestamp.  Unless you have changed it for your own surveys, Qualtrics data is collected in Mountain time.  Therefore, I have indicated the US/Mountain timezone.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#df$DateVariable <- ymd_hms(df$DateVariable, tz = "US/Mountain")
```


#### What if I care about time zones?

As noted in the prior question, "tz" is the function used to indicate the time zone.  A complete list of timezones can be found with the *OlsonNames()* function. I have hashtagged it out because it is quite a length list!

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#OlsonNames()
```


```{r include = FALSE}
sessionInfo()
```


