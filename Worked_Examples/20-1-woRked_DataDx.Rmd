```{r include = FALSE}
options(scipen=999)
```


## Homeworked Example
[Screencast Link]()

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introductory lesson](https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html#introduction-to-the-data-set-used-for-homeworked-examples) in [ReCentering Psych Stats](https://lhbikos.github.io/ReCenterPsychStats/). An .rds file which holds the data is located in the [Worked Examples](https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples) folder at the GitHub site the hosts the OER. The file name is *ReC.rds*.

Although the lessons focused on preparing data for analyses were presented in smaller sections, this homeworked example combines the suggestions for practice from the [Scrubbing](#scrub), [Scoring](#scrub), and [Data Dx](#datadx) lessons. My hope is that is cumulative presentation is a closer approximation of what researchers need for their research projects.

These lessons were created to prepare a set of data to analyze a specific research model. Consequently, the model should be known and described at the beginning.

### Scrubbing

#### Specify a research model  {-}

A further requirement was that the model should include three predictor variables (continuously or categorically scaled) and one dependent (continuously scaled) variable.

I am hypothesizing that socially responsive pedagogy (my dependent variable) will increase as a function of:

* the transition from SPSS (0) to R(1),
* the transition from a pre-centered (0) to re-centered (1) curriculum, and
* higher evaluations of traditional pedagogy

Because this data is nested within the person (i.e., students can contribute up to three course evaluations over the ANOVA, multivariate, and psychometrics courses) proper analysis would require a statistic (e.g., multilevel modeling) that would address the dependency in the data. Therefore, I will include only those students who are taking the multivariate modeling class.

*If you wanted to use this example and dataset as a basis for a homework assignment, you could create a different subset of data. I worked the example for students taking the multivariate modeling class. You could choose ANOVA or psychometrics. You could also choose a different combinations of variables.*


![An image of our the prediction model for the homeworked example.](Worked_Examples/images/homeworked_model.jpg)

#### Import data {-}

```{r}
raw <- readRDS("ReC.rds")
nrow(raw)
```

#### Include only those who consented {-} 

Because this data is publicly posted on the Open Science Framework, it was necessary for me to already exclude those individuals. This data was unique in that students could freely write some version of "Opt out." My original code included a handful of versions, but here was the basic form:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#testing to see if my code worked
#raw <- dplyr::filter (raw, SPFC.Decolonize.Opt.Out != "Okay")
raw <- dplyr::filter (raw, SPFC.Decolonize.Opt.Out != "Opt Out")
```


#### Apply exclusionary criteria {-} 

I want to exclude students' responses for the ANOVA and psychometrics courses.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <-(dplyr::filter (raw, Course == "Multivariate"))

```
At this point, these my only inclusion/exclusion criteria. I can determine how many students (who consented) completed any portion of the survey.

```{r}
nrow(raw)
```

#### Rename variables to be sensible and systematic {-} 

Because this dataset is already on the OSF, the variables are sensibly named. However, I don't like "SPFC.Decolonize.Opt.Out". I will change it to simply "OptOut."

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <- dplyr::rename(raw, OptOut = 'SPFC.Decolonize.Opt.Out')
```

It would have made more sense to do this before I used this variable in the calculations. 

#### Downsize the dataframe to the variables of interest {-} 

I will need to include:

* deID
* StatsPkg
* Centering
* Items included in the traditional pedagogy scale: ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation 
* Items included in the socially responsive pedagogy scale: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scrub_df <-(dplyr::select (raw, deID, StatsPkg, Centering,ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration))
```

#### Provide an APA style write-up of these preliminary steps {-} 

>>This is a secondary analysis of data involved in a more comprehensive dataset that included students taking multiple statistics courses (*N* = 310). Having retrieved this data from a repository in the Open Science Framework, only those who consented to participation in the study were included. Data used in these analyses were 84 students who completed the multivariate clas. 

### Scoring

#### Proper formatting of the item(s) in your first predictor variable {-} 

StatsPkg is a dichotomous variable. It should be structured as a factor with two ordered levels:  SPSS, R

Because I am using the .rds form of the data from the OSF, this variable retains the former structure I assigned to it. If I needed to write the code, I would do this:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scrub_df$StatsPkg <- factor(scrub_df$StatsPkg, levels = c("SPSS", "R"))
str(scrub_df$StatsPkg)
```

#### Proper formatting of item(s) in your second predictor variable  {-} 

Similarly, Centering is a dichotomous variable. It should be structured as a factor with two ordered levels:  Pre, Re. 

Because I am using the .rds form of the data from the OSF, this variable retains the former structure I assigned to it. If I needed to write the code, I would do this:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scrub_df$Centering <- factor(scrub_df$Centering, levels = c("Pre", "Re"))
str(scrub_df$Centering)
```

#### Proper formatting of the item(s) in your third predictor variable {-} 
#### Proper formatting of the item(s) in your dependent variable {-} 

The third predictor variable is traditional pedagogy. The dependent variable is socially repsonsive pedagogy. The items that will be used in the scale scores for both of these variables are all continuously scaled and should be identified as "int" or "num." None of the items need to be reverse-scored.

```{r}
str(scrub_df)
```

#### Evaluate and interpret item-level missingness {-} 

The *scrub_df* is already downsized to include the item-level raw variables and the ID variable. We can continue using it.

I will create a "proportion missing" variable.

In this chunk I first calculate the number of missing (nmiss)

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)#needed because the script has pipes

#Calculating number and proportion of item-level missingness
scrub_df$nmiss <- scrub_df%>%
    dplyr::select(StatsPkg:DEIintegration) %>% #the colon allows us to include all variables between the two listed (the variables need to be in order)
    is.na %>% 
    rowSums

scrub_df<- scrub_df%>%
  dplyr::mutate(prop_miss = (nmiss/11)*100) #11 is the number of variables included in calculating the proportion
```
We can grab the descriptives for the *prop_miss* variable to begin to understand our data.  I will create an object from it so I can use it with inline
```{r }
psych::describe(scrub_df$prop_miss)
```
Because I want to use the AIA approach to scoring, I'm not willing to filter out any cases yet. If I wanted to eliminate cases with egregious missing (i.e., like 90%), here is the code I would use:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scrub_df <- dplyr::filter(scrub_df, prop_miss <= 90)  #update df to have only those with at least 90% of complete data
```


CUMULATIVE CAPTURE FOR WRITING IT UP:  

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36%.

To analyze missingness at this level, we need a df that has only the variables of interest.  That is, variables like *ID* and the *prop_miss* and *nmiss* variables we created will interfere with an accurate assessment of missingness. I will update our df to eliminate these.  

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#further update to exclude the n_miss and prop_miss variables
ItemMiss_df <- scrub_df %>%
  dplyr::select (-c(deID, nmiss, prop_miss))
```

Missing data analysis commonly looks at proportions by:

* the entire df
* rows/cases/people

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#what proportion of cells missing across entire dataset
formattable::percent(mean(is.na(ItemMiss_df)))
#what proportion of cases (rows) are complete (nonmissing)
formattable::percent(mean(complete.cases(ItemMiss_df)))

```

CUMULATIVE CAPTURE FOR WRITING IT UP: 

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36%.  Across the dataset, 2.38% of cells had missing data and 82.14% of cases had nonmissing data.

We can further explore patterns of missingness with *mice.md.pattern*.


```{r eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
mice::md.pattern(ItemMiss_df, plot = TRUE, rotate.names = TRUE)

```

There are 6 missingness patterns. The most common (*n* = 69) have no missingness. There are 11 students missing the DEIintegration item (on the traditional pedagogy scale). This item may have been a later addition to the Canvas course evaluations.

Comparing this to Enders' [-@enders_applied_2010] [prototypical patterns of missingness](https://www.google.com/books/edition/Applied_Missing_Data_Analysis/uHt4EAAAQBAJ?hl=en&gbpv=1&dq=enders+missing+data&pg=PP1&printsec=frontcover) (page 3), the *mice* output represents the monotonic pattern often caused by test fatigue. That is, once a student stopped responding, they didn't continue with the rest of the evaluation. That said, this was true of only 4 students (1 each pattern). A quick reminder -- diagnosing monotonicity requires that the variables in the *mice.mdpattern* figures were presented to the research participant in that order.

#### Score any scales/subscales {-} 

Traditional pedagogy is a predictor variable that needs to be created by calculating the mean if at least 75% of the items are non-missing. None of the items need to be reverse-scored. I will return to working with the *scrub_df* data.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#this seems to work when I build the book, but not in "working the problem"
#TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')  
#scrub_df$TradPed <- sjstats::mean_n(scrub_df[, TradPed_vars], .75)

#this seems to work when I "work the problem" (but not when I build the book)
#the difference is the two dots before the last SRPed_vars
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')  
scrub_df$TradPed <- sjstats::mean_n(scrub_df[, TradPed_vars], .75)
```

The dependent variable is socially responsive pedagogy. It needs to be created by calculating the mean if at least 75% of the items are non-missing. None of the items need to be reverse-scored.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#this seems to work when I build the book, but not in "working the problem"
#SRPed_vars <- c('InclusvClassrm','EquitableEval', 'MultPerspectives', 'DEIintegration')  
#scrub_df$SRPed <- sjstats::mean_n(scrub_df[, SRPed_vars], .75)

#this seems to work when I "work the problem" (but not when I build the book)
#the difference is the two dots before the last SRPed_vars
SRPed_vars <- c('InclusvClassrm','EquitableEval', 'MultPerspectives', 'DEIintegration')  
scrub_df$SRPed <- sjstats::mean_n(scrub_df[, SRPed_vars], .75)
```
#### Evaluate and interpret scale-level missingness {-} 

To evaluate scale level missingness, let's create a df with the focal variables.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scored <- dplyr::select (scrub_df, StatsPkg, Centering, TradPed, SRPed)
ScoredCaseMiss <- nrow(scored) #I produced this object for the sole purpose of feeding the number of cases into the inline text, below
ScoredCaseMiss
```

Before we start our formal analysis of missingness at the scale level, let's continue to scrub by eliminating cases that will have too much missingness. In the script below we create a variable that counts the number of missing variables and then creates a proportion by dividing it by the number of total variables.

Using the *describe()* function from the *psych* package, we can investigate this variable.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
#Create a variable (n_miss) that counts the number missing
scored$n_miss <- scored%>%
 is.na %>% 
rowSums

#Create a proportion missing by dividing n_miss by the total number of variables (6)
#Pipe to sort in order of descending frequency to get a sense of the missingness
scored<- scored%>%
mutate(prop_miss = (n_miss/6)*100)%>%
  arrange(desc(n_miss))

psych::describe(scored$prop_miss)

```
CUMULATIVE CAPTURE FOR WRITING IT UP:  

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36%.  Across the dataset, 2.38% of cells had missing data and 82.14% of cases had nonmissing data.

>>Across the 84 cases for which the scoring protocol was applied, missingness ranged from 0 to 33%.

We need to decide what is our retention threshhold. Twenty percent seems to be a general rule of thumb.  Let's delete all cases with missingness at 20% or greater.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#update df to have only those with at least 20% of complete data (this is an arbitrary decision)
scored <- filter(scored, prop_miss <= 20) 

#the variable selection just lops off the proportion missing
scored <-(select (scored, StatsPkg:SRPed)) 

#this produces the number of cases retained
nrow(scored)
```

CUMULATIVE CAPTURE FOR WRITING IT UP:  

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 100%.  Across the dataset, 3.86% of cells had missing data and 87.88% of cases had nonmissing data.

>>Across the 84 cases for which the scoring protocol was applied, missingness ranged from 0 to 67%. After eliminating cases with greater than 20% missing, the dataset analyzed included 83 cases. 

Now, at the scale level, we look at missingness as the proportion of 

* individual cells across the scored dataset, and
* rows/cases with nonmissing data

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}

#percent missing across df
formattable::percent(mean(is.na(scored)))
#percent of rows with nonmissing data
formattable::percent(mean(complete.cases(scored))) 

```

CUMULATIVE CAPTURE FOR WRITING IT UP: 

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 100%.  Across the dataset, 3.86% of cells had missing data and 87.88% of cases had nonmissing data.

>>Across the 84 cases for which the scoring protocol was applied, missingness ranged from 0 to 67%. After eliminating cases with greater than 20% missing, the dataset analyzed included 83 cases. In this dataset we had less than 1% (0.60%) missing across the df; 98% of the rows had nonmissing data.

Let's look again at missing patterns and mechanisms.

Returning to the *mice* package, we can use the *md.pattern()* function to examine a matrix with the number of columns 1 in which each row corresponds to a missing data pattern (0 = observed, 0 = missing). The rows and columns are sorted in increasing amounts of missing information. The last column and row contain row and column counts, respectively.

The corresponding figure shows non-missing data in blue; missing data in red.

```{r message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
mice_ScaleLvl <- mice::md.pattern(scored, plot = TRUE, rotate.names=TRUE)
mice_ScaleLvl
```

There are *2* rows of data because there are only *2* patterns of missingness. The most common pattern is non-missing data (*n* = 81). Two cases are missing the SRPed variable. If our statistical choice uses listwise deletion (i.e., the case is eliminated if one or more variables in the model has missing data), our sample size will be 79. As we will earn in later chapters, there are alternatives (i.e., specifying a FIML option in analyses that use maximum likelihood estimators) that can use all of the cases -- even those with missing data. 

#### Represent your work in an APA-style write-up (added to the writeup in the previous chapter {-} 

>>Available item analysis (AIA; [@parent_handling_2013]) is a strategy for managing missing data that uses available data for analysis and excludes cases with missing data points only for analyses in which the data points would be directly involved. Parent (2013) suggested that AIA is equivalent to more complex methods (e.g., multiple imputation) across a number of variations of sample size, magnitude of associations among items, and degree of missingness. Thus, we utilized Parent’s recommendations to guide our approach to managing missing data. Missing data analyses were conducted with tools in base R as well as the R packages, *psych* (v. 2.3.6) and *mice* (v. 3.16.0). 

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 100%.  Across the dataset, 3.86% of cells had missing data and 87.88% of cases had nonmissing data.

>>Across the 84 cases for which the scoring protocol was applied, missingness ranged from 0 to 67%. After eliminating cases with greater than 20% missing, the dataset analyzed included 83 cases. In this dataset we had less than 1% (0.60%) missing across the df; 98% of the rows had nonmissing data.

### Data Dx

#### Calculate alpha coefficients for scales/subscales {-} 

To calculate the alpha coefficients, we need item-level data. We will return to *scrub_df* that contains the item-level data.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#alpha for the traditional pedagogy scale
psych::alpha(scrub_df[c("ClearResponsibilities", "EffectiveAnswers","Feedback", "ClearOrganization","ClearPresentation")])
```
>>Cronbach's alpha for the traditional pedagogy scale was 0.88.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#alpha for the traditional pedagogy scale
psych::alpha(scrub_df[c("InclusvClassrm", "EquitableEval", "DEIintegration", "DEIintegration")])
```

>>Cronbach's alpha for the socially responsive pedagogy scale was 0.85.

Both of these are above the recommended value of 0.80.


#### Evaluate univariate normality (skew, kurtosis, Shapiro-Wilks) {-} 

We can inspect univariate normality by examining the skew and kurtosis values of the continuously scored variables.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describe(scored, type=1)
```

When we use the "type=1" argument, the skew and kurtosis indices in the *psych* package can be interpreted according to Kline's [-@kline_data_2016] guidelines. 

>>Regarding the distributional characteristics of the data, skew and kurtosis values for our continuously scaled variables fall below the thresholds of concern (i.e., absolute value of 3 for skew; absolute value of 10 for kurtosis) identified by Kline [-@kline_data_2016] as concerning.

Still at the univariate level, we can apply the Shapiro-Wilk test of normality to each of our continuously scaled variables. When the $p$ value is < .05, the variable's distribution is deviates from a normal distribution to a degree that is statistically significant. Below, the plotting of the histogram with a normal curve superimposed shows how the distribution approximates one that is normal.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#The shapiro-test is in base R; it's specification is simple:  shapiro.test(df$variable)
#I added the object (and had to list it below) so I can use the inline text function
shapiro.test(scored$TradPed)
shapiro.test(scored$SRPed)

```

Both variable differ from a normal distribution in a statistically significant way.

* For the traditional pedagogy variable, $W = 0.830, p < 0.001$
* for the socially responsive pedagogy variable, $0.818, p < 0.001$

Obtaining a quick *psych::pairs.panel* can provide a quick glimpse of the distribution.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::pairs.panels(scored, stars = TRUE, lm = TRUE)
```

CUMULATIVE CAPTURE FOR THE APA STYLE WRITE-UP:  

>>Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 10 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. Results of the Shapiro-Wilk test of normality indicate that our variables assessing the traditional pedagogy ($W = 0.830, p < 0.001$) and socially responsive pedagogy (0.818, p < 0.001) are statistically significantly different than a normal distribution. Inspection of distributions of the variables indicated that both course evaluation variables were negatively skewed, with a large proportion of high scores.

#### Evaluate multivarite normality (Mahalanobis test) {-} 

In more complex models, multivariate normality is probably a more useful analysis. Although I am teaching this evaluation in advance of the formal analysis, as demonstrated in many of [ReCentering Psych Stats ANOVA chapters](https://lhbikos.github.io/ReCenterPsychStats/analysis-of-variance.html), this can also be assessed by examining the distribution of residuals after the analysis is complete.

Multivariate normality can be assessed with the continuously scaled variables. The code below includes the only two continuously scaled variables.  The code simultaneously (a) appends the df with a Mahalanobis value and (b) creates a QQ plot. Dots that stray from the line are the scores that are contributing to multivariate non-normality.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scored$Mahal <- psych::outlier(scored[c("TradPed", "SRPed")]) 
```

We can analyze the distributional characteristics of the Mahalanobis values with *psych::describe*.
It is possible, then to analyze the Mahalanobis distance values.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describe(scored$Mahal)
```
Using this information we can determine cases that have a Mahalanobis distance values that exceeds three standard deviations around the median.  In fact, we can have these noted in a column in the dataframe.

```{r warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70) }
#creates a variable indicating TRUE or FALSE if an item is an outlier
scored$MOutlier <- dplyr::if_else(scored$Mahal > (median(scored$Mahal) + (3*sd(scored$Mahal))), TRUE, FALSE)

#shows us the first 6 rows of the data so we can see the new variables (Mahal, MOutlier)
head(scored)

```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
#counts frequency TRUE and FALSE indicating outlier or not
OutlierCount<- scored%>%
  dplyr::count(MOutlier)

#calculating how many outliers a slightly different way
nrow(scored) - OutlierCount 
```
When we identify outliers we often ask if we should delete them or transform the data. A general rule of thumb is to look for "jumps" in the Mahalanobis distance values. If they are progressing steadily and there is no "jump," researchers will often retain the outliers.

In this case, I do see a jump. When I sort the df on Mahal values, the jump from 9.37 to 16.56 is much different than the more gradual increase in values that precedes it. Therefore, I think I will delete cases with Mahalanobis values greater than 10 (a number I "just picked").

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
scored <- dplyr::filter (scored, Mahal < "10")
```


>>We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *psych::outlier()* function  and included both continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that 2 cases exceed three standard deviations beyond the median. Because there was a substantial "jump" between the non-outliers and these two variables we chose to delete them. 

#### Represent your work in an APA-style write-up (added to the writeup in the previous chapter) {-} 

>>This is a secondary analysis of data involved in a more comprehensive dataset that included students taking multiple statistics courses (*N* = 310). Having retrieved this data from a repository in the Open Science Framework, only those who consented to participation in the study were included. Data used in these analyses were 84 students who completed the multivariate clas. 

>>Available item analysis (AIA; [@parent_handling_2013]) is a strategy for managing missing data that uses available data for analysis and excludes cases with missing data points only for analyses in which the data points would be directly involved. Parent (2013) suggested that AIA is equivalent to more complex methods (e.g., multiple imputation) across a number of variations of sample size, magnitude of associations among items, and degree of missingness. Thus, we utilized Parent’s recommendations to guide our approach to managing missing data. Missing data analyses were conducted with tools in base R as well as the R packages, *psych* (v. 2.3.6) and *mice* (v. 3.16.0). 

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 100%.  Across the dataset, 3.86% of cells had missing data and 87.88% of cases had nonmissing data.

>>Across the 84 cases for which the scoring protocol was applied, missingness ranged from 0 to 67%. After eliminating cases with greater than 20% missing, the dataset analyzed included 83 cases. In this dataset we had less than 1% (0.60%) missing across the df; 98% of the rows had nonmissing data.

>>Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 10 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. Results of the Shapiro-Wilk test of normality indicate that our variables assessing the traditional pedagogy ($W = 0.830, p < 0.001$) and socially responsive pedagogy (0.818, p < 0.001) are statistically significantly different than a normal distribution. Inspection of distributions of the variables indicated that both course evaluation variables were negatively skewed, with a large proportion of high scores.

>>We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *psych::outlier()* function  and included both continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that 2 cases exceed three standard deviations beyond the median. Because there was a substantial "jump" between the non-outliers and these two variables we chose to delete them. 

#### Conduct a quick analysis (e.g., regression, ANOVA) including at least three variables {-} 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
SRPed_fit <- lm(SRPed ~ StatsPkg + Centering + TradPed, data = scored)
summary(SRPed_fit)
```
### Results

>>Results of a multiple regression predicting the socially responsive course evaluation ratings indicated that neither the transition from SPSS to R ($B = 0.133, p = 0.105$) nor the transition to an explicitly recentered curriculum ($B = 0.057, p = 0.448) led to statistically significant diferences. In contrast, traditional pedagogy had a strong, positive effect on evaluations of socially responsive pedagogy ($B = 0.686, p < 0.001). The model accounted for 62% of the variance and was statistically significant ($p , 0.001$). Means, standard deviations, and correlations among variables are presented in Table 1; results of the regression model are presented in Table 2.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
apaTables::apa.cor.table(scored[c("SRPed", "StatsPkg", "Centering", "TradPed")], table.number = 1, show.sig.stars = TRUE, filename = "Table1__DataDx_HW.doc")
```
