```{r include = FALSE}
options(scipen=999)
```


## Homeworked Example
[Screencast Link]()

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introductory lesson](https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html#introduction-to-the-data-set-used-for-homeworked-examples) in [ReCentering Psych Stats](https://lhbikos.github.io/ReCenterPsychStats/). An .rds file which holds the data is located in the [Worked Examples](https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples) folder at the GitHub site the hosts the OER. The file name is *ReC.rds*.

Although the lessons focused on preparing data for analyses were presented in smaller sections, this homeworked example combines the suggestions for practice from the [Scrubbing](#scrub), [Scoring](#scrub), [Data Dx](#datadx) because they are also used when missing data is managed with multiple imputation. My hope is that is cumulative presentation is a closer approximation of what researchers need for their research projects.

These lessons were created to prepare a set of data to analyze a specific research model. Consequently, the model should be known and described at the beginning.

### Scrubbing

#### Specify a research model  {-}

A further assignment requirement was that the model should include three predictor variables (continuously or categorically scaled) and one dependent (continuously scaled) variable.

As in the homeworked example for the Data Dx lesson, I am hypothesizing that socially responsive pedagogy (my dependent variable) will increase as a function of:

* the transition from SPSS (0) to R(1),
* the transition from a pre-centered (0) to re-centered (1) curriculum, and
* higher evaluations of traditional pedagogy

Because this data is nested within the person (i.e., students can contribute up to three course evaluations over the ANOVA, multivariate, and psychometrics courses) proper analysis would require a statistic (e.g., multilevel modeling) that would address the dependency in the data. Therefore, I will include only those students who are taking the multivariate modeling class.

While it is possible to conduct multiple imputation at the scale level, we will do so at the item-level (i.e., before we compute the scale scores).

*If you wanted to use this example and dataset as a basis for a homework assignment, you could create a different subset of data. I worked the example for students taking the multivariate modeling class. You could choose ANOVA or psychometrics. You could also choose a different combinations of variables.*


![An image of our the prediction model for the homeworked example.](Worked_Examples/images/homeworked_model.jpg)

#### Import data {-}

```{r}
raw <- readRDS("ReC.rds")
nrow(raw)
```

#### Apply inclusionary/exclusionary criteria {-} 

Because this data is publicly posted on the Open Science Framework, it was necessary for me to already exclude those individuals. This data was unique in that students could freely write some version of "Opt out." My original code included a handful of versions, but here was the basic form:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#testing to see if my code worked
#raw <- dplyr::filter (raw, SPFC.Decolonize.Opt.Out != "Okay")
raw <- dplyr::filter (raw, SPFC.Decolonize.Opt.Out != "Opt Out")
```

I want to exclude students' responses for the ANOVA and psychometrics courses.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <-(dplyr::filter (raw, Course == "Multivariate"))

```
At this point, these my only inclusion/exclusion criteria. I can determine how many students (who consented) completed any portion of the survey.

```{r}
nrow(raw)
```

#### Format any variables that shouldn't be imputed in their raw form    
*(
Let's first create a df with the item-level variables that will fuel our model. In addition to the variables in our model, we will include four auxillary variables. These include Dept (Department: Clinical or Industrial-Organizational) and four additional course evaluation items:  OvInstructor, MyContribution, IncrInterest, IncrUnderstanding. 

Let's check the structure to be certain that *StatsPkg* (SPSS, R) and  *Centered* (Pre, Re) are ordered factors. We also want the course evaluation items to be integer (or numerical).  

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
mimp_df <-(dplyr::select (raw, deID, StatsPkg, Centering,ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration, Dept, OvInstructor, MyContribution, IncrInterest, IncrUnderstanding))
str(mimp_df)
```

We should eliminate case with greater than 50% missingness.  
```{r warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
#Calculating number and proportion of item-level missingness
mimp_df$nmiss <- mimp_df%>%
    select(StatsPkg:IncrUnderstanding) %>% #the colon allows us to include all variables between the two listed (the variables need to be in order)
    is.na %>% 
    rowSums

mimp_df<- mimp_df%>%
  mutate(prop_miss = (nmiss/13)*100) #11 is the number of variables included in calculating the proportion

mimp_df <- filter(mimp_df, prop_miss <= 50)  #update df to have only those with at least 50% of complete data
```

Once again, trim the df to include only the data to be included in the imputation
```{r atidy=TRUE, tidy.opts=list(width.cutoff=70)}
mimp_df <-  select (mimp_df, deID, StatsPkg, Centering,ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration, Dept, OvInstructor, MyContribution, IncrInterest, IncrUnderstanding)
```


#### Multiply impute a minimum of 5 sets of data   

Because multiple imputation is a *random* process, if we all want the same answers we need to set a *random seed.* 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
set.seed(2309034) #you can pick any number you want, today I'm using today's datestamp
```

The program we will use is *mice*. *mice* assumes that each variable has a distribution and it imputes missing variables according to that distribution.  

This means we need to correctly specify each variable's format/role.  *mice* will automatically choose a distribution (think "format") for each variable; we can override this by changing the methods' characteristics.

The following code sets up the structure for the imputation. This follows the Katitas example.

```{r warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(mice)
# runs the mice code with 0 iterations
imp <- mice(mimp_df, maxit = 0)
# Extract predictor Matrix and methods of imputation 
predM = imp$predictorMatrix
meth = imp$method
log = imp$log
```

Here we code what format/role each variable should be.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#These variables are left in the dataset, but setting them = 0 means they are not used as predictors.  
#We want our ID to be retained in the df.  There's nothing missing from it, and we don't want it used as a predictor, so it will just hang out.
predM[, c("deID")]=0

#If you like, view the first few rows of the predictor matrix
#head(predM)

#We don't have any ordered categorical variables, but if we did we would follow this format
#poly <- c("Var1", "Var2")

#We have three dichotomous variables
log <- c("StatsPkg", "Centering", "Dept")

#Unordered categorical variables (nominal variables), but if we did we would follow this format
#poly2 <- c("format")

#Turn their methods matrix into the specified imputation models
#Remove the hashtag if you have any of these variables
#meth[poly] = "polr" 
meth[log] = "logreg"
#meth[poly2] = "polyreg"

meth
```


This list (meth) contains all our variables; "pmm" is the default and is the "predictive mean matching" process used. We see that *StatsPkg* and *Centering* are noted as "logreg." This is because they are dichotomous variables.  If there is *""* underneath it means the data is complete. The data will be used in imputing other data, but none of that data will be imputed.

Our variables of interest are now configured to be imputed with the imputation method we specified.  Empty cells in the method matrix mean that those variables aren't going to be imputed.

If a variable has no missing values, it is automatically set to be empty.  We can also manually set variables to not be imputed with the *meth[variable]=""* command.

The code below begins the imputation process. We are asking for 5 datasets. If you have many cases and many variables, this can take awhile. How many imputations?  Recommendations have ranged as low as five to several hundred.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# With this command, we tell mice to impute the anesimpor2 data, create 5vvdatasets, use predM as the predictor matrix and don't print the imputation process. 
#If you would like to see the process (or if the process is failing to execute) set print as TRUE; seeing where the execution halts can point to problematic variables (more notes at end of lecture)

imp2 <- mice(mimp_df, maxit = 5, 
             predictorMatrix = predM, 
             method = meth, 
             log = log,
             print =  FALSE)
```

We need to create a "long file" that stacks all the imputed data.  Looking at the df in R Studio shows us that when imp = 0 (the pe-imputed data), there is still missingness.  As we scroll through the remaining imputations, there are no NA cells.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# First, turn the datasets into long format
# This procedure is, best I can tell, unique to mice and wouldn't work for repeated measures designs
mimp_long <- mice::complete(imp2, action="long", include = TRUE)
```

If we look at it, we can see 6 sets of data. If the *deID* variable is sorted we see that:

* .imp = 0 is the unimputed set; there are still missing values
* .imp = 1, 2, 3, or 5 has no missing values for the variables we included in the imputation

With the code below we can see the proportion of missingness for each variable (that has missing data), sorted from highest to lowest.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
p_missing_mimp_long <- unlist(lapply(mimp_long, function(x) sum(is.na(x))))/nrow(mimp_long)
sort(p_missing_mimp_long[p_missing_mimp_long > 0], decreasing = TRUE)#check to see if this works
```
Because our imputation was item-level, we need to score the variables with scales/subscales.

Traditional pedagogy is a predictor variable that needs to be created by calculating the mean if at least 75% of the items are non-missing. None of the items need to be reverse-scored. I will return to working with the *scrub_df* data.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#this seems to work when I build the book, but not in "working the problem"
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')  
#mimp_long$TradPed <- sjstats::mean_n(mimp_long[, TradPed_vars], .75)

#this seems to work when I "work the problem" (but not when I build the book)
#the difference is the two dots before the last SRPed_vars
mimp_long$TradPed <- sjstats::mean_n(mimp_long[, TradPed_vars], .75)
```

The dependent variable is socially responsive pedagogy. It needs to be created by calculating the mean if at least 75% of the items are non-missing. None of the items need to be reverse-scored.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#this seems to work when I build the book, but not in "working the problem"
#SRPed_vars <- c('InclusvClassrm','EquitableEval', 'MultPerspectives', 'DEIintegration')  
#mimp_long$SRPed <- sjstats::mean_n(mimp_long[, SRPed_vars], .75)

#this seems to work when I "work the problem" (but not when I build the book)
#the difference is the two dots before the last SRPed_vars
SRPed_vars <- c('InclusvClassrm','EquitableEval', 'MultPerspectives', 'DEIintegration')  
mimp_long$SRPed <- sjstats::mean_n(mimp_long[, SRPed_vars], .75)
```


#### Run a regression (for multiply imputed data) with at least three variables

For comparison, here was the script when we used the AIA approach for managing missingness:

>>SRPed_fit <- lm(SRPed ~ StatsPkg + Centering + TradPed, data = scored)

In order for the regression to use multiply imputed data, it must be a "mids" (multiply imputed data sets) type
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Convert to mids type - mice can work with this type
mimp_mids<-as.mids(mimp_long)
```

Here's what we do with imputed data:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
fitimp <- with(mimp_mids,
               lm(SRPed ~ StatsPkg + Centering + TradPed))
```

In this process, 5 individual, OLS, regressions are being conducted and the results being pooled into this single set.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# to get the 5, individual imputations
summary(fitimp)
```


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
summary(pool(fitimp))
```
>>Results of a multiple regression predicting the socially responsive course evaluation ratings indicated that neither the transition from SPSS to R ($B = 0.178, p = 0.135$) nor the transition to an explicitly recentered curriculum ($B = 0.116, p = 0.285) led to statistically significant diferences. In contrast, traditional pedagogy had a strong, positive effect on evaluations of socially responsive pedagogy ($B = 0.571, p < 0.001). Results of the regression model are presented in Table 2.


#### APA style write-up of the multiple imputation section of data diagnostics     

My write-up draws from some of the results we obtained in the homeworked example at the end of the [Data Dx](#DataDx) chapter.

>>This is a secondary analysis of data involved in a more comprehensive dataset that included students taking multiple statistics courses (*N* = 310). Having retrieved this data from a repository in the Open Science Framework, only those who consented to participation in the study were included. Data used in these analyses were 84 students who completed the multivariate clas. 

>>Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 100%.  Across the dataset, 3.86% of cells had missing data and 87.88% of cases had nonmissing data. At this stage in the analysis, missingness for all cases did not exceed 50% [@katitas_getting_2019] and they were all included in the multiple imputation .

>>Regarding the distributional characteristics of the data, skew and kurtosis values of the variables fell below the values of 3 (skew) and 10 (kurtosis) that Kline suggests are concerning [-@kline_principles_2016]. Results of the Shapiro-Wilk test of normality indicate that our variables assessing the traditional pedagogy ($W = 0.830, p < 0.001$) and socially responsive pedagogy (0.818, p < 0.001) are statistically significantly different than a normal distribution. Inspection of distributions of the variables indicated that both course evaluation variables were negatively skewed, with a large proportion of high scores.

>>We evaluated multivariate normality with the Mahalanobis distance test. Specifically, we used the *psych::outlier()* function  and included both continuous variables in the calculation. Our visual inspection of the Q-Q plot suggested that the plotted line strayed from the straight line as the quantiles increased.  Additionally, we appended the Mahalanobis distance scores as a variable to the data. Analyzing this variable, we found that 2 cases exceed three standard deviations beyond the median. 

>>We managed missing data with multiple imputation [@enders_multiple_2017; @katitas_getting_2019]. We imputed five sets of data with the R package, *mice* (v. 3.13) -- a program that utilizes conditional multiple imputation. The imputation included the 9 item-level variables that comprised our scales and the dichotomous variable representing traditional pedagogy and socially responsive pedagogy. We also included five auxiliary variables (four variables from the course evaluation and the whether the student was from the Clinical or Industrial-Organizational Psychology program).

#### AAPA style write-up regression results

>>Results of a multiple regression predicting the socially responsive course evaluation ratings indicated that neither the transition from SPSS to R ($B = 0.178, p = 0.135$) nor the transition to an explicitly recentered curriculum ($B = 0.116, p = 0.285) led to statistically significant diferences. In contrast, traditional pedagogy had a strong, positive effect on evaluations of socially responsive pedagogy ($B = 0.571, p < 0.001). Results of the regression model are presented in Table 2.

*As in the lesson itself, I used the data diagnostics that we did in the AIA method.  It feels to me like these should be calculated with the multiply imputed data (i.e., 5 sets, with pooled estimates and standard errors), but I do not see that modeled -- anywhere in tutorials I consulted.*

```{r include=FALSE}
sessionInfo()
```
