```{r include = FALSE}
options(scipen=999)
```

## Homeworked Example
[Screencast Link](https://youtu.be/gnzO7fpCL3A)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introductory lesson](https://lhbikos.github.io/ReCenterPsychStats/ReCintro.html#introduction-to-the-data-set-used-for-homeworked-examples) in [ReCentering Psych Stats](https://lhbikos.github.io/ReCenterPsychStats/). An .rds file which holds the data is located in the [Worked Examples](https://github.com/lhbikos/ReC_MultivModel/tree/main/Worked_Examples) folder at the GitHub site the hosts the OER. The file name is *ReC.rds*.

The suggested practice problem for this chapter is to evaluate the measurement model that would precede the evaluation of a structural model. And actually, we will need to evaluate two measurement models -- an "all items" on indicators model and a parceled model.


###  Identify the structural model you will evaluate {-} 

It should have a minimum of three variables and could be one of the prior path-level models you already examined   
 
I will repeat the simple mediation that I suggested in path analysis. Specifically, I hypothesize that the evaluation of socially responsive pedagogy will be predicted by intentional recentering through traditional pedagogy.

X = Centering: explicit recentering (0 = precentered; 1 = recentered)
M = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable)
Y = SRPed:  socially responsive pedagogy (continuously scaled with higher scores being more favorable)

![An image of the our hypothesized structural model](Worked_Examples/images/HW_ReC1_theo_plot.png) 

### Specify a research model {-}

I am hypothesizing that the evaluation of social responsive pedagogy is predicted by intentional recentering through traditional pedagogy. 

### Import the data and format the variables in the model {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
raw <- readRDS("ReC.rds")
```

I don't need to score my scales, but it is important to know what they are:

TradPed has 5 items:  ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation
SRPed has 4 items: InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration
Centering is 1 item -- it's a factor with two levels pre, re.

I can create a babydf with just those items.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf <- dplyr::select(raw, Centering, ClearResponsibilities, EffectiveAnswers, Feedback, ClearOrganization, ClearPresentation, InclusvClassrm, EquitableEval, MultPerspectives, DEIintegration)
```

Let's check the structure of the variables:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
str(babydf)
```
The centering variable will need to be dummy coded as 0/1:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
babydf$CENTERING <- as.numeric(babydf$Centering)
babydf$CENTERING <- (babydf$CENTERING - 1)
str(babydf)
```

### Specify and evaluate a measurement model that you have established {-} 

That is, it is not necessary to repeat all the steps of the prior lesson's instructions to first specify a model with all indicators and then compare it to one that is more parsimonious. In the prior lesson, I decdied that the unique characteristics of the items (5 indicators for TradPED, 4 indicators for SRPed) I would use all indicators for each variable. Here is a diagram of what we are specifying:

![An image of the measurement model we will specify](Worked_Examples/images/ReC_msmt_diag.png) 

As shown below, the centering variable is a single item indicator.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
msmt_mod <- "
        ##measurement model
         CTR =~ CENTERING #this is a single item indicator, I had to add code below to set the variance

         TradPed =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
         
         SRPed =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         
        # Variance of the single item indicator
        CENTERING ~~ 0*CENTERING
        
        # Covariances
         CTR ~~ TradPed
         CTR ~~ SRPed
         TradPed ~~ SRPed
        "

set.seed(230916)
msmt_fit <- lavaan::cfa(msmt_mod, data = babydf, missing = "fiml")
msmt_fit_sum <- lavaan::summary(msmt_fit, fit.measures = TRUE, standardized = TRUE)
msmt_fit_sum
msmt_fit_pEsts <- lavaan::parameterEstimates(msmt_fit, boot.ci.type = "bca.simple", standardized=TRUE)
#msmt_fit_pEsts #To reduce redundancy in the book, I did not print the parameter estimates. Their object is used in exporting a .csv file.

```

Below is script that will export the global fit indices (via *tidySEM::table_fit*) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.  
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
msmt_globalfit <- tidySEM::table_fit(msmt_fit)
write.csv(msmt_globalfit, file = "msmt_globalfit.csv")
#the code below writes the parameter estimates into a .csv file
write.csv(msmt_fit_pEsts, file = "msmt_fit_pEsts.csv")
```

Here is how I wrote up the results:

>Analyzing our proposed multiple mediator model followed the two-step procedure of first evaluating a measurement model with acceptable fit to the data and then proceeding to test the structural model. Given that different researchers recommend somewhat differing thresholds to determine the adequacy of fit, We used the following as evidence of good fit: comparative fit indix (CFI) $\geq 0.95$, root-mean-square error of approximation (RMSEA) $\leq 0.06$, and the standard root-mean-square residual (SRMR) $\leq 0.08$. To establish aceptable fit, we used CFI $\geq 0.90$, RMSEA $\leq 0.10$, and SRMR $\leq 0.10$ [@weston_brief_2006].

>We evaluated the measurement model by following recommendations by Little et al. [@little_parcel_2002; @little_why_2013]. Specifically,each latent variable was represented by each of the items on its subscale. Given that TradPed and SRPed had 5 and 4 items, respectively, we did not parcel items. The Centering variable with two levels (pre-centered, re-centered) was recoded as a dummy variable with 0, 1 coding. In the specification, its measurement error was fixed at zero. While all factor loadings were strong, statistically significant, and properly valanced, global fit statistics were mixed: $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$. Factor loadings of each of the parcels are presented in Table 1. We proceeded to testing the strutural model with caution.


Table 1  

|Factor Loadings for the Measurement Model 
|:-------------------------------------------------------------|

|                         
|:----------------------------:|:-----:|:----:|:-----:|:------:|
| Latent variable and indicator|est    |SE    | *p*   |est_std |

|
|:-----------------------------|:-----:|:----:|:-----:|:------:|
|**Traditional Pedagogy**      |       |      |       |        |
|ClearResponsibilities	       |1.000	 |0.000	|      	|0.845   |
|EffectiveAnswers	             |0.967	 |0.056	|<0.001	|0.815   |
|Feedback	                     |0.915	 |0.063	|<0.001	|0.725   |
|ClearOrganization	           |1.193	 |0.075	|<0.001	|0.771   |
|ClearPresentation	           |1.111	 |0.063	|<0.001	|0.841   |
|**Socially Responsive Pedagogy**|     |      |       |        |
|InclusvClassrm	               |1.000	 |0.000	|     	|0.702   |
|EquitableEval	               |0.953	 |0.087	|<0.001 |0.717   |
|MultPerspectives	             |1.466	 |0.116	|<0.001	|0.839   |
|DEIintegration	               |0.901	 |0.099	|<0.001	|0.582   |
|**CENTERING**	               |0.000	 |0.000	|       |0.000   |


Here is a figure of my measurement model:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
semPlot::semPaths(msmt_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```
```{r eval=FALSE, echo=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Code to plot the theoretical model (in case you don't want to print the results on the graph):
p_msmt_diag <- semPlot::semPaths(msmt_fit)

```
### Specify and evaluate a *structural* model {-}

As a reminder, I hypothesize that the evaluation of socially responsive pedagogy will be predicted by intentional recentering through traditional pedagogy.

X = Centering: explicit recentering (0 = precentered; 1 = recentered)
M = TradPed: traditional pedagogy (continuously scaled with higher scores being more favorable)
Y = SRPed:  socially responsive pedagogy (continuously scaled with higher scores being more favorable)

![An image of the our hypothesized structural model](Worked_Examples/images/HW_ReC1_theo_plot.png) 


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ReC_struct_mod <- "
        #measurement model
        CTR =~ CENTERING #this is a single item indicator, I had to add code below to set the variance
        TradPed =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
        SRPed =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         
        # Variance of the single item indicator
        CENTERING ~~ 0*CENTERING
         
        #structural model with labels for calculation of the indirect effect
         SRPed ~ b*TradPed + c_p*CTR 
         TradPed ~a*CTR
          
        #calculations
         indirect :=  a*b
         direct  := c_p
         total_c  := c_p + (a*b)
          "
set.seed(230916) #needed for reproducibility especially when specifying bootstrapped confidence intervals
ReC_struct_fit <- lavaan::sem(ReC_struct_mod, data = babydf, missing= 'fiml')
ReC_struct_summary <- lavaan::summary(ReC_struct_fit, fit.measures=TRUE, standardized=TRUE, rsq = TRUE)
ReC_struct_pEsts <- lavaan::parameterEstimates(ReC_struct_fit, boot.ci.type = "bca.simple", standardized=TRUE)
ReC_struct_summary
#ReC_struct_pEsts #although creating the object is useful to export as a .csv I didn't ask it to print into the book

```

Below is script that will export the global fit indices (via *tidySEM::table_fit*) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.  
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
ReC_globalfit <- tidySEM::table_fit(ReC_struct_fit)
write.csv(ReC_globalfit, file = "ReC_globalfit.csv")
#the code below writes the parameter estimates into a .csv file
write.csv(ReC_struct_pEsts, file = "ReC_struct_pEsts.csv")
```

I will compare my results to the global fit criteria/thresholds:

Criteria                                            | Our Results                            | Criteria met?|
|:--------------------------------------------------|:--------------------------------------:|:------------:|
|Non-significant chi-square                         |$\chi ^{2}(33) = 178.307, p = < 0.001$  |No            |  
|$CFI\geq .95$ (or at least .90)                    |CFI = 0.911                             |Just barely   |  
|$RMSEA\leq .05$ (or < .08, at least < .10, also 90CI)|RMSEA = 0.119, 90CI[0.102, 0.137]     |No           |  
|$SRMR\leq .08$ (at least < .10)                    |SRMR = 0.060                            |Yes           | 
|Combination rule: $CFI \geq .95$ & $SRMR \leq .08$ |CFI = 0.911, SRMR = 0.060               |No            |

Tabling the factor loadings and parameter estimates provide an organized way to examine local fit.

**Table 2 ** 

|Model Coefficients Assessing the Effect of Recentering on Socially Responsive Pedagogy Through Traditional Pedagogy
|:--------------------------------------------------------------------------------------|

| Predictor                        |$B$     |$SE_{B}$|$p$      |$\beta$ |$R^2$          |                   
|:---------------------------------|:------:|:------:|:-------:|:-------|:-------------:|
|Traditional Pedagogy (M)          |        |        |         |        |.01            |  
|Centering ($a$)                   |-0.105	|0.084	 |0.215	   |-0.074  |               |
|Socially Responsive Pedagogy (DV) |        |        |         |        |.75            |     
|Traditional Pedagogy ($b$)        |0.595	  |0.050	 |0.000	   |0.865   |               |
|Centering ($c'$)                  |0.095	  |0.042	 |0.025	   |0.098   |               |

|Effects                           |$B$     |$SE_{B}$|$p$      |        |95% CI 
|:---------------------------------|:------:|:------:|:-------:|:------:|:-------------:|
|Indirect($a*b$)                   |-0.062	|0.050	 |0.216	   |-0.064	|-0.161, 0.037  |
|Total effect                      |0.033	  |0.061	 |0.595	   |0.034	  |-0.088, 0.153  |

|
|---------------------------------------------------------------------------------------|
|*Note*. The significance of the indirect effects was calculated with bootstrapped, bias-corrected, confidence intervals (.95).|

For a little extra leaRning, I compared these SEM results to the path analytic ones from the homeworked lesson on simple mediation. While the values are not identical, they are parallel. No indirect effect was found there, either.

Here's how I might write up this section of the results:

> Like our measurement model, our structural model is just-identified with zero degrees of freedom. Consequently, the global fit indices are identical and provided an inconsistent evaluation of fit: $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$. Indices of local fit (e.g., regression weights, parameter estimates) are presented in Table 2 and Figure 1 provides a graphical representation of our results. While results suggest that Centering $(B = 0.095, p = 0.025)$ traditional pedagogy $(B = 0.595, p < 0.001)$ had statistically significant effects on socially responsive pedagogy, there was no evidence of an indirect effect $(B = -0.062, SE = 0.050, p = 0.216)$. The model accounted for only 1% of the variance in traditional pedagogy and 75% of the variance in socially responsive pedagogy.  

Let's work up a figure
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
plot_ReC_struct <- semPlot::semPaths(ReC_struct_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```


|Grid for Plotting semplot::sempath  
|:-------------|:-------------|:------------|
|(1,1) empty   |(1,2) TrP     |(1,3) empty  | 
|(2,1) CTR     |(2,2) empty   |(2,3) SRP    |


We place these values along with the names of our latent variables in to the *semptools::layout_matrix* function.

Lots of things can go wrong in the code below. In preparing this example I lost time 

* because I could not distinguish between the capital "I" (i) and the lowercase "l" (L) in the SemPlot, and
* because in the *m1_indicator_factor* code I did not list the latent variables enough time to match the number of indicators in the *m1_indicator_order*

After traversing several rabbit trails, taking a break, and returning, I could see my errors.  That's just R.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#IMPORTANT:  Must use the node names (take directly from the SemPlot) assigned by SemPlot
#You can change them as the last thing
m1_msmt <- semptools::layout_matrix(CTR = c(2,1),
                                  TrP = c(1,2),
                                  SRP = c(2,3))

#tell where you want the indicators to face
m1_point_to <- semptools::layout_matrix (left = c(2,1),
                                      up = c(1,2),
                                      right = c(2,3))

#the next two codes -- indicator_order and indicator_factor are paired together, they specify the order of observed variables for each factor
m1_indicator_order <- c("ClR", "EfA", "Fdb", "ClO", "ClP",
                     "InC", "EqE", "MlP", "DEI",
                    "CEN")

m1_indicator_factor <- c("TrP", "TrP", "TrP", "TrP", "TrP",
                      "SRP", "SRP", "SRP", "SRP",
                      "CTR")

#next set of code pushes the indicator variables away from the factor
m1_indicator_push <- c(CTR = 1, 
                    TrP = 2,
                    SRP = 5)

#spreading the boxes away from each other
m1_indicator_spread <- c(CTR = 1, 
                    TrP = 3,
                    SRP = 5)

```

Finally, we can feed all of the objects that whole these instructions into the *semptools::sem_set_layout* function. If desired, we can use the *semptools::change_node_label* function to rename the latent variables. Again, make sure to use the variable names that *semPlot::semPaths* has assigned.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
plot1 <- semptools::set_sem_layout(plot_ReC_struct,
                                indicator_order = m1_indicator_order,
                                indicator_factor = m1_indicator_factor,
                                factor_layout = m1_msmt,
                                factor_point_to = m1_point_to,
                                indicator_push = m1_indicator_push,
                                indicator_spread = m1_indicator_spread)

#changing node labels
plot1 <- semptools::change_node_label(plot1,
                                   c(CTR = "Centering",
                                     TrP = "TradPed",
                                     SRP = "SRPed"),
                                   label.cex = 1.1)

plot(plot1)
```

```{r eval=FALSE, echo=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Code to plot the theoretical model (in case you don't want to print the results on the graph):
ReC1_theo <- semPlot::semPaths(ReC_struct_fit)
ReC1_theo <-semptools::set_sem_layout(ReC1_theo,
                                indicator_order = m1_indicator_order,
                                indicator_factor = m1_indicator_factor,
                                factor_layout = m1_msmt,
                                factor_point_to = m1_point_to,
                                indicator_push = m1_indicator_push,
                                indicator_spread = m1_indicator_spread)
plot(ReC1_theo)
```


###  Respecify and evaluate an *alternative* structural model {-}

Having worked with this data quite a bit, traditional and socially responsive pedagogy are highly correlated. Thus, I think I would like to see how centering predicts both traditional pedagogy and socially responsive pedagogy. In this analysis, I want to fix the covariance between TradPed and SRPed to be 0.0. Why?  I'm *just curious* to see what will happen.

![An image of the our alternative structural model](Worked_Examples/images/HW_ReC1_alt_plot.png) 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ReC_ALT_mod <- "
        #measurement model
        CTR =~ CENTERING #this is a single item indicator, I had to add code below to set the variance
        TradPed =~ ClearResponsibilities + EffectiveAnswers + Feedback + ClearOrganization + ClearPresentation
        SRPed =~ InclusvClassrm + EquitableEval + MultPerspectives + DEIintegration
         
        # Variance of the single item indicator
        CENTERING ~~ 0*CENTERING
        TradPed~~0*SRPed
         
        #structural model with labels for calculation of the indirect effect
         SRPed ~ CTR 
         TradPed ~ CTR
          
          "
set.seed(230916) #needed for reproducibility especially when specifying bootstrapped confidence intervals
ReC_ALT_fit <- lavaan::sem(ReC_ALT_mod, data = babydf, missing= 'fiml', fixed.x= FALSE)
ReC_ALT_summary <- lavaan::summary(ReC_ALT_fit, fit.measures=TRUE, standardized=TRUE, rsq = TRUE)
ReC_ALT_pEsts <- lavaan::parameterEstimates(ReC_ALT_fit, boot.ci.type = "bca.simple", standardized=TRUE)
ReC_ALT_summary
#ReC_ALT_pEsts #although creating the object is useful to export as a .csv I didn't ask it to print into the book

```

Below is script that will export the global fit indices (via *tidySEM::table_fit*) and the parameter estimates (e.g., factor loadings, structural regression weights, and parameters we requested such as the indirect effect) to .csv files that you can manipulate outside of R.  
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#global fit indices
ReC_ALT_globalfit <- tidySEM::table_fit(ReC_ALT_fit)
write.csv(ReC_ALT_globalfit, file = "ReC_ALT_globalfit.csv")
#the code below writes the parameter estimates into a .csv file
write.csv(ReC_ALT_pEsts, file = "ReC_ALT_pEsts.csv")
```

I will compare my results to the global fit criteria/thresholds:

Criteria                                            | Our Results                            | Criteria met?|
|:--------------------------------------------------|:--------------------------------------:|:------------:|
|Non-significant chi-square                         |$\chi ^{2}(34) = 414.408, p = < 0.001$  |No            |  
|$CFI\geq .95$ (or at least .90)                    |CFI = 0.767                             |No            |  
|$RMSEA\leq .05$ (or < .08, at least < .10, also 90CI)|RMSEA = 0.190, 90CI[0.174, 0.207]     |No            |  
|$SRMR\leq .08$ (at least < .10)                    |SRMR = 0.278                            |No            | 
|Combination rule: $CFI \geq .95$ & $SRMR \leq .08$ |CFI = 0.767, SRMR = 0.278               |No            |

Tabling the factor loadings and parameter estimates provide an organized way to examine local fit.

**Table 2 ** 

|Model Coefficients Assessing the Effect of Recentering on Socially Responsive Pedagogy Through Traditional Pedagogy
|:-------------------------------------------------------------------------------|

|Predictor                         |$B$     |$SE_{B}$|$p$      |$\beta$ |$R^2$   |                   
|:---------------------------------|:------:|:------:|:-------:|:-------|:------:|
|Traditional Pedagogy (M)          |        |        |         |        |.01     |  
|Centering                         |-0.106	|0.084	 |0.210	   |-0.075  |        |
|Socially Responsive Pedagogy (DV) |        |        |         |        |.01     |     
|Centering                         |0.077	  |0.070	 |0.271	   |0.071   |        |


Here's how I might write up this section of the results:

>In structural models where there is a cross-sectional analysis, the flow of causation is ambiguous. Therefore, we tested an alternative model, predicting both traditional and socially responsive pedagogy from centering status. The global fit indices of this alternative model suggested that fit was poor, $\chi^2(34) = 414.408, p = < 0.00, CFI =  0.767, RMSEA = 0.190, 90CI[0.174, 0.207], SRMR = 0.278$. Indices of local fit (e.g., regression weights, parameter estimates) are presented in Table 3 and Figure 2 provides a graphical representation of our results. Centering had non-significant effects on traditional pedagogy $(B = -0.106, p = 0.210)$ and socially responsive pedagogy $(B = 0.077, p < 0.271)$ Further, the model accounted for only 1% of the variance each, in traditional and socially responsive pedagogy.  


Let's work up a figure
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
plot_ReC_ALT <- semPlot::semPaths(ReC_ALT_fit, what = "col", whatLabels = "stand", sizeMan = 5, node.width = 1, edge.label.cex = .75, style = "lisrel", mar = c(5,5,5,5))
```


|Grid for Plotting semplot::sempath  
|:-------------|:-------------|
|(1,1) empty   |(1,2) TrP     |
|(2,1) CTR     |(2,2) empty   |
|(3,1) empty   |(3,2) SRP     |


We place these values along with the names of our latent variables in to the *semptools::layout_matrix* function.

Lots of things can go wrong in the code below. In preparing this example I lost time 

* because I could not distinguish between the capital "I" (i) and the lowercase "l" (L) in the SemPlot, and
* because in the *m1_indicator_factor* code I did not list the latent variables enough time to match the number of indicators in the *m1_indicator_order*

After traversing several rabbit trails, taking a break, and returning, I could see my errors.  That's just R.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#IMPORTANT:  Must use the node names (take directly from the SemPlot) assigned by SemPlot
#You can change them as the last thing
m2_msmt <- semptools::layout_matrix(CTR = c(2,1),
                                  TrP = c(1,2),
                                  SRP = c(3,2))

#tell where you want the indicators to face
m2_point_to <- semptools::layout_matrix (left = c(2,1),
                                      right = c(1,2),
                                      right = c(3,2))

#the next two codes -- indicator_order and indicator_factor are paired together, they specify the order of observed variables for each factor
m2_indicator_order <- c("ClR", "EfA", "Fdb", "ClO", "ClP",
                     "InC", "EqE", "MlP", "DEI",
                    "CEN")

m2_indicator_factor <- c("TrP", "TrP", "TrP", "TrP", "TrP",
                      "SRP", "SRP", "SRP", "SRP",
                      "CTR")

#next set of code pushes the indicator variables away from the factor
m2_indicator_push <- c(CTR = 1, 
                    TrP = 2,
                    SRP = 5)

#spreading the boxes away from each other
m2_indicator_spread <- c(CTR = 1, 
                    TrP = 3,
                    SRP = 5)

```

Finally, we can feed all of the objects that whole these instructions into the *semptools::sem_set_layout* function. If desired, we can use the *semptools::change_node_label* function to rename the latent variables. Again, make sure to use the variable names that *semPlot::semPaths* has assigned.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
plot2 <- semptools::set_sem_layout(plot_ReC_ALT,
                                indicator_order = m2_indicator_order,
                                indicator_factor = m2_indicator_factor,
                                factor_layout = m2_msmt,
                                factor_point_to = m2_point_to,
                                indicator_push = m2_indicator_push,
                                indicator_spread = m2_indicator_spread)

#changing node labels
plot21 <- semptools::change_node_label(plot2,
                                   c(CTR = "Centering",
                                     TrP = "TradPed",
                                     SRP = "SRPed"),
                                   label.cex = 1.1)

plot(plot2)
```


```{r eval=FALSE, echo=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Code to plot the theoretical model (in case you don't want to print the results on the graph):
ReC_ALT <- semPlot::semPaths(ReC_ALT_fit)
ReC_ALT <-semptools::set_sem_layout(ReC_ALT,
                                indicator_order = m2_indicator_order,
                                indicator_factor = m2_indicator_factor,
                                factor_layout = m2_msmt,
                                factor_point_to = m2_point_to,
                                indicator_push = m2_indicator_push,
                                indicator_spread = m2_indicator_spread)
plot(ReC_ALT)
```
### Conduct a formal comparison of *global* fit.

We can use the *lavaan::lavTestLRT* to compare the global fit of the models.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
lavaan::lavTestLRT(msmt_fit, ReC_struct_fit, ReC_ALT_fit)
```
As predicted, the fit of the measurement and first structural models are identical. This is because they were both just-identified with zero degrees of freedom. The fit of the alternative model is statistically significantly different from the structural model. Results of the $\Delta \chi^2$ test were statistically significant: $\Delta \chi^2(1) = 236.1, p < 0.001$. We know from evaluating the global fit indices that the initial structural model (i.e., the simple mediation) is preferred.

### APA style results with table and figure {-}    

>**Method/Analytic Strategy**

>We specified a structural equation model predicting socially responsive pedagogy (SRPed) from centering status (Centering), mediated by traditional pedagogy (TradPed). The primary analysis occurred in two stages. First, we specified and evaluated a measurement model Data were analyzed with a maximum likelihood approach the package, *lavaan* (v. 0.6-16). 

>**Results**

>**Preliminary Analyses**

>*  Missing data analyses and managing missing data>
>*  Bivariate correlations, means, SDs
>*  Distributional characteristics, assumptions, etc.
>*  Address limitations and concerns

>**Primary Analyses**
>Analyzing our proposed multiple mediator model followed the two-step procedure of first evaluating a measurement model with acceptable fit to the data and then proceeding to test the structural model. Given that different researchers recommend somewhat differing thresholds to determine the adequacy of fit, We used the following as evidence of good fit: comparative fit indix (CFI) $\geq 0.95$, root-mean-square error of approximation (RMSEA) $\leq 0.06$, and the standard root-mean-square residual (SRMR) $\leq 0.08$. To establish aceptable fit, we used CFI $\geq 0.90$, RMSEA $\leq 0.10$, and SRMR $\leq 0.10$ [@weston_brief_2006].

>We evaluated the measurement model by following recommendations by Little et al. [@little_parcel_2002; @little_why_2013]. Specifically,each latent variable was represented by each of the items on its subscale. Given that TradPed and SRPed had 5 and 4 items, respectively, we did not parcel items. The Centering variable with two levels (pre-centered, re-centered) was recoded as a dummy variable with 0, 1 coding. In the specification, its measurement error was fixed at zero. While all factor loadings were strong, statistically significant, and properly valanced, global fit statistics were mixed: $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$. Factor loadings of each of the parcels are presented in Table 1. We proceeded to testing the strutural model with caution.

> Like our measurement model, our structural model is just-identified with zero degrees of freedom. Consequently, the global fit indices are identical and provided an inconsistent evaluation of fit: $\chi^2(33)= 178.307, p < 0.001, CFI =  0.911, RMSEA = 0.119, 90CI[0.102, 0.137], SRMR = 0.060$. Indices of local fit (e.g., regression weights, parameter estimates) are presented in Table 2 and Figure 1 provides a graphical representation of our results. While results suggest that Centering $(B = 0.095, p = 0.025)$ traditional pedagogy $(B = 0.595, p < 0.001)$ had statistically significant effects on socially responsive pedagogy, there was no evidence of an indirect effect $(B = -0.062, SE = 0.050, p = 0.216)$. The model accounted for only 1% of the variance in traditional pedagogy and 75% of the variance in socially responsive pedagogy.  

>In structural models where there is a cross-sectional analysis, the flow of causation is ambiguous. Therefore, we tested an alternative model, predicting both traditional and socially responsive pedagogy from centering status. The global fit indices of this alternative model suggested that fit was poor, $\chi^2(34) = 414.408, p = < 0.00, CFI =  0.767, RMSEA = 0.190, 90CI[0.174, 0.207], SRMR = 0.278$. Indices of local fit (e.g., regression weights, parameter estimates) are presented in Table 3 and Figure 2 provides a graphical representation of our results. Centering had non-significant effects on traditional pedagogy $(B = -0.106, p = 0.210)$ and socially responsive pedagogy $(B = 0.077, p < 0.271)$ Further, the model accounted for only 1% of the variance each, in traditional and socially responsive pedagogy.  

>To formally compare the fit of the initial structural and alternative models we conducted a $\Delta \chi^2$ difference test. Resultstest were statistically significant: $\Delta \chi^2(1) = 236.1, p < 0.001$. We know from evaluating the global fit indices that the initial structural model (i.e., the simple mediation) is preferred.


### Explanation to grader {-}


### A homework suggestion {-}

There is one thing you could do to further improve this model's fit. Perhaps it could be one of your homework options!


